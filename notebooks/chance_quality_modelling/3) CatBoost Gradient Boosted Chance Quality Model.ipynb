{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) CatBoost Gradient Boosted Chance Quality Model from Shots Data\n",
    "##### Notebook to further improve the performance of the Chance Quality Model (CQM) model created in the previous notebook, from a provided sample of just under 11,000 shots, through the application of the CatBoost Gradient Boosting algorithm.\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 02/05/2021<br>\n",
    "Notebook last updated: 02/05/2021\n",
    "\n",
    "![title](../../img/expected_goals_visual.png)\n",
    "\n",
    "Photo credit to David Sumpter ([@Soccermatics](https://twitter.com/Soccermatics?))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='introduction'>Introduction</a>\n",
    "This notebook is a short walk-through of how to create an Expected Goals (xG) model using a just under 11,00 shots, in [Python](https://www.python.org/), using [pandas](http://pandas.pydata.org/) DataFrames, [scikit-learn](https://scikit-learn.org/stable/) and [CatBoost](https://catboost.ai/) for Machine Learning, [matplotlib](https://matplotlib.org/contents.html?v=20200411155018) visualisations, and [SHAP](https://shap.readthedocs.io/en/latest/) for feature importance.\n",
    "\n",
    "For more information about this notebook and the author, I am available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/); and\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster).\n",
    "\n",
    "![title](../../img/edd_webster/fifa21eddwebsterbanner.png)\n",
    "\n",
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/mcfc_submission/) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/mcfc_submission/blob/main/notebooks/chance_quality_modelling/Creating%20a%20Chance%20Quality%20Model%20from%20Shots%20Data.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.      [Notebook Dependencies](#section1)<br>\n",
    "2.      [Project Brief](#section2)<br>\n",
    "3.      [Introduction to CatBoost](#section3)<br>\n",
    "4.      [Data Sources](#section4)<br>\n",
    "        1.    [Data Dictionary](#section4.1)<br>\n",
    "        2.    [Creating the DataFrame](#section4.2)<br>\n",
    "        3.    [Initial Data Handling](#section4.3)<br>    \n",
    "5.      [Initial Modeling](#section5)<br>\n",
    "6.      [k-fold Cross Validation using CatBoost](#section6)<br>\n",
    "7.      [Feature Importance with CatBoost](#section7)<br>\n",
    "8.      [Visualisation of CatBoost Tree](#section8)<br>\n",
    "9.      [Hyperparameter Optimisation](#section9)<br>\n",
    "10.     [Final Optimised CatBoost Model](#section10)<br>\n",
    "11.     [Performance comparison of CatBoost with XGBoost and Logistic Regression](#section11)<br>\n",
    "12.     [Assessment of the Performance of the Teams in Game 2 of the Metrica Sports Shot Data](#section12)<br>\n",
    "13.     [Summary](#section13)<br>\n",
    "14.     [Next Steps](#section14)<br>\n",
    "15.     [References and Further Reading](#section15)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section1'>1. Notebook Dependencies</a>\n",
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing;\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation;\n",
    "*    [`matplotlib`](https://matplotlib.org/contents.html?v=20200411155018) for data visualisations; and\n",
    "*    [`scikit-learn`](https://scikit-learn.org/stable/index.html) for Machine Learning.\n",
    "\n",
    "All packages used for this notebook except can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python ≥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "from os.path import basename\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib.patches import Arc\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import ruamel.yaml\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno as msno\n",
    "#from xgboost import plot_tree\n",
    "#import graphviz\n",
    "\n",
    "# Downloading data sources\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Football libraries\n",
    "#import FCPython\n",
    "#from FCPython import createPitch\n",
    "#import matplotsoccer\n",
    "\n",
    "# Machine Learning\n",
    "import scipy as sp\n",
    "from scipy.spatial import distance\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "#from sklearn.inspection import permutation_importance\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score , roc_curve, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from scikitplot.metrics import plot_roc_curve, plot_precision_recall_curve, plot_calibration_curve\n",
    "import pickle\n",
    "import shap\n",
    "#import lightgbm as lgb\n",
    "#import xgboost as xgb\n",
    "#from xgboost import XGBClassifier, cv\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier, Pool, cv, MetricVisualizer\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, Video, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "print('Seaborn: {}'.format(sns.__version__))\n",
    "print('Plotly: {}'.format(plotly.__version__))\n",
    "print('CatBoost: {}'.format(catboost.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_shots = os.path.join(base_dir, 'data', 'shots')\n",
    "data_dir_metrica = os.path.join(base_dir, 'data', 'metrica-sports')\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "models_dir_shots = os.path.join(base_dir, 'models', 'shots')\n",
    "scripts_dir = os.path.join(base_dir, 'scripts')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "fig_shots_dir = os.path.join(base_dir, 'img', 'fig', 'shots')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple timer function to time algorithms\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns of pandas DataFrames\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section2'>2. Project Brief</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section2.1'>2.1. About this notebook</a>\n",
    "In most real-world machine-learning tasks, a well-calibrated gradient booster such as [CatBoost](https://catboost.ai/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/) or [XGBoost](https://xgboost.readthedocs.io/en/latest/) outperforms Logistic Regression when using the same set of features.\n",
    "\n",
    "Often, smartly, hand-crafted features can bring a Logistic Regression to an (almost) similiar performance as when using Gradient Boosting algorithms. This is especially true when dealing with rather simple problems with a relatively small number of features.\n",
    "\n",
    "Using the engineered dataset in the first notebook, the following sections build upon this modeling, using gradient boosting algorithms, to try and further reduce the Log Loss of the model and improve upon the potential predictions made upon the Metrica Sports data, to analyse which team was more deserving to with the game based solely on the chances created.\n",
    "\n",
    "This notebook builds upon the work conducted in the first Chance Quality notebook, created using a trained Logistic Regression model, and to further improve the performance using [CatBoost](https://catboost.ai/). The model uses the engineered dataset of 11,000 shots dervied in the previous notebook, using [pandas](http://pandas.pydata.org/) DataFrames for data manipulation, [matplotlib](https://matplotlib.org/contents.html?v=20200411155018) for data visualisation, and [SHAP](https://shap.readthedocs.io/en/latest/) for feature importance.\n",
    "\n",
    "**Notebook Conventions**:<br>\n",
    "*    Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "*    Variables that refer to a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section2.2'>2.2. Challenge</a>\n",
    "Defined in the previous notebook but included here as a reminders of the big picture.\n",
    "\n",
    "<b>Step 1:</b>\n",
    "We have attached to this email a sample of just under 11,000 shots (ShotData.csv, and the associated description in ShotData.txt). We would like you to use this data to build a chance quality model that calculates the probability of a shot resulting in a goal (i.e. P[goal|shot,situation]) using whichever situational variables in the data you think are informative. We ask that you provide a description of the method that you chose, including any metrics and plots that you have used to understand and assess the performance of your model. This description may take the form of a short written report (no more than one page of text plus additional room for figures & tables) or a slide pack (PowerPoint, Google slides, etc; no more than a total of 10 slides).\n",
    "\n",
    "<b>Step 2:</b>\n",
    "In the second step we ask you to work with the tracking data for a single game to analyse the shooting opportunities that each team created. In this github repository* you will find the tracking data for two matches, along with a description of the data. Using the data for sample game 2 in the repository, identify the shots in this game and write a short report describing the major chances that each team created during the game, making use of the chance quality model that you developed in Step 1 and any other information that you think is relevant. Based solely on the quality of chances that each team created, which team do you think deserved to win the game? Your report may take the form of a document (1 page plus additional room for figures & tables) or presentation (no more than 10 slides).\n",
    "\n",
    "This notebook is concerned with <b>Step 1</b> - building a Chance Quality Model, using [CatBoost](https://catboost.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section2.3'>2.3. What is CatBoost?</a>\n",
    "CatBoost is ...\n",
    "\n",
    "This notebook aims to hopefully serve as an explanatory guide of [CatBoost](https://catboost.ai/) for beginners and those with previous experience alike and goes into much more detail about this algorithm in the follow sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section2.4'>2.4. Modelling Approach</a>\n",
    "This model will build upon the work done in the first notebook in the series, that builds a Chance Quality Model from Logistic Regression and looks to improve on the results using the popular XGBoost algorithm. The approach taken in this notebook can be defined as the following:\n",
    "\n",
    "*    <b>Introduction to CatBoost</b>: introduction to the concept of XGBoost ([section 3](#section3));\n",
    "*    <b>Data Sources</b>:  ([section 4](#section4));\n",
    "*    <b>Initial Modeling</b>: First model created as a baseline for which iterations of improvement are based. ([section 5](#section5));\n",
    "*    <b>k-fold Cross Validation using CatBoost</b>: ([section 8](#section8));\n",
    "*    <b>Feature Importance with CatBoost</b>: ([section 9](#section9));\n",
    "*    <b>Visualisation of CatBoost Tree</b>: ([section 10](#section10));\n",
    "*    <b>Hyperparameter Optimisation</b>: ([section 11](#section11));\n",
    "*    <b>Final Optimised CatBoost Model</b>: ([section 12](#section12));\n",
    "*    <b>Performance Comparison of CatBoost with XGBoost and Logistic Regression</b>: ([section 13](#section13));\n",
    "*    <b>Assessment of the Performance of the Teams in Game 2 of the Metrica Sports Shot Data</b>: ([section 14](#section14));\n",
    "*    <b>Summary</b>: ([section 15](#section15));\n",
    "*    <b>Next Steps</b>: ([section 16](#section16)); and\n",
    "*    <b>References and Further Reading</b>: ([section 17](#section17))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section3'>3. Introduction to CatBoost</a>\n",
    "*    \n",
    "*    \n",
    "*    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section4'>4. Data Sources</a>\n",
    "The following cells read in the engineered Shots data as a CSV file, created in the first Chance Quality Model notebook [[link](https://github.com/eddwebster/mcfc_submission/blob/main/notebooks/chance_quality_modelling/Creating%20a%20Chance%20Quality%20Model%20from%20Shots%20Data.ipynb)]. The original dataset of just under 11,000 shots was provided by [Laurie Shaw](https://twitter.com/EightyFivePoint) from [City Football Group](https://www.cityfootballgroup.com/) (see docs [[link](https://github.com/eddwebster/mcfc_submission/blob/main/documentation/shots/ShotData.txt)])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.1'>4.1. Data Dictionary</a>\n",
    "The following information is as per the definition in the `ShotData.txt` documentation, provided with the data [[link](https://github.com/eddwebster/mcfc_submission/blob/main/documentation/shots/ShotData.txt)].\n",
    "\n",
    "The engineered shots DataFrame the following features:\n",
    "\n",
    "| Feature                           | Original/Engineered?     | Variables Type     | Data Type    | Description    |\n",
    "|-----------------------------------|--------------------------|--------------------|--------------|----------------------------------------------------|\n",
    "| `match_minute`                    | original                 | continuous         | int64        | Minute of the match in which the shot was taken     |\n",
    "| `match_second`                    | original                 | continuous         | int64        | Second of match_minute in which the shot was taken     |\n",
    "| `position_x`                      | original                 | continuous         | float64      | Position of the shot on the pitch in meters (x-coordinate)     |\n",
    "| `position_y`                      | original                 | continuous         | float64      | Position of the shot on the pitch in meters (y-coordinate)     |\n",
    "| `play_type`                       | original                 | categorical        | object       | Game situation in which the shot was taken (open play, penalty, direct free kick, direct from a corner)     |\n",
    "| `BodyPart`                        | original                 | categorical        | object       | Body part with which shot was taken (left foot, right foot, head, other)                        |\n",
    "| `Number_Intervening_Opponents`    | original                 | discrete           | int64        | The number of opposing players that were obscuring the goal at the instant of the shot (from the perspective of the shot-taker)     |\n",
    "| `Number_Intervening_Teammates`    | original                 | discrete           | int64        | The number of teammates that are obscuring the goal at the instant of the shot (from the perspective of the shot-taker)     |\n",
    "| `Interference_on_Shooter`         | original                 | categorical        | object       | The degree of direct interference exerted on the shot-taker from defenders (Low - no or minimal interference, Medium - a single defender was in close proximity to the shot-taker; High - multiple defenders in close proximity and interfering with the shot).     |\n",
    "| `outcome`                         | original                 | discrete           | object       | The outcome of the shot (blocked, missed, goal frame (post or bar), saved, goal or own goal).     |\n",
    "| `position_xM`                     | engineered               | discrete           | float64      | Converted position of the shot along the x-axis, derived from the `position_x` feature for x(-53, +53) dimensions.     |\n",
    "| `position_yM`                     | engineered               | discrete           | float64      | Converted position of the shot along the y-axis, derived from the `position_y` feature for y(-34, +34) dimensions.     |\n",
    "| `isGoal`                          | engineered               | discrete           | object       | Indicates whether the resulting shot was a goal, or not. Derived from the `outcome` feature. Used as the target variable.     |\n",
    "| `distance_to_goalM`               | engineered               | continuous         | float64      | Distance in which the shot is from the goal, in meters. Detemined from the `position_xM` and `position_yM` features.    |\n",
    "| `distance_to_centerM`             | engineered               | continuous         | float64      | Distance in which the shot is from the center of the pitch, in meters. Detemined from the `position_yM` feature.     |\n",
    "| `angle`                           | engineered               | discrete           | float64      | Angle in which the taken shot is to the goal, in degrees. Detemined from the `position_xM` and `position_yM` features.     |\n",
    "| `isFoot`                          | engineered               | discrete           | int64        | Indicates whether the resulting shot was taken with the foot - yes (1) or no (0).     |\n",
    "| `isHead`                          | engineered               | discrete           | int64        | Indicates whether the resulting shot was taken with the head - yes (1) or no (0).     |\n",
    "| `header_distance_to_goalM`        | engineered               | continuous         | float64      | Distance in which the headed shot is from the goal, in meters. Detemined from the `position_xM` and `position_yM` feature.     |\n",
    "| `High`                            | engineered               | discrete           | int64        | One-hot/dummy encoding of the `Interference_on_Shooter` feature. Indicates whether the shot experiences a high level of intereference (multiple defenders in close proximity and interfering with the shot) - yes (1) or no (0).      |\n",
    "| `Medium`                             | engineered               | discrete           | int64        | One-hot/dummy encoding of the `Interference_on_Shooter` feature. Indicates whether the shot experiences a medium level of intereference (a single defender was in close proximity to the shot-taker) - yes (1) or no (0).      |\n",
    "| `Low`                          | engineered               | discrete           | int64        | One-hot/dummy encoding of the `Interference_on_Shooter` feature. Indicates whether the shot experiences a low level of intereference (no or minimal interference) - yes (1) or no (0).      |\n",
    "\n",
    "The dataset also contains the following columns, that are only used for data visualisation purposes only and not considered for the model (some of these may be deleted from the dataset later on after this notebook is further tidied): `position_yM`, `position_xM_r`, `position_yM_r`, `position_xM_std`, `position_yM_std`, `position_xM_std_r`, `position_yM_std_r`, `Interference_on_Shooter_Code`, and `BodyPartCode`.\n",
    "\n",
    "Each row consists of a single shot event.\n",
    "\n",
    "The following data engineering of the raw shots dataset took place in the previous notebook:\n",
    "*    Dataset of 10,925 shots filtered for Open Play shots only, leaving 10,269 Open-Play shots before data engineering (656 shots removed, corresponding to 156 goals).\n",
    "*    Filtered out shots where `BodyPart` is 'Other' to prevent confusion when using this trained model with the Metrica Sports data where the body part with which the shot is taken is always known. 58 shots removed from the Open Play dataset, corresponding to 12 goals.\n",
    "*    Filtered out shots where `Interference_on_Shooter` is 'Unknown' to prevent confusion when using this trained model with the Metrica Sports data where the Interference on the shooter is always known as it is calculated. 25 shots removed from the Open Play dataset, corresponding to 9 goals.\n",
    "*    As part of the outlier removal, replaced the values of shots taken from goal to no goal, where the `distance_to_goalM` greater than 35m (38.27 yards) or where the `distance_to_goalM` is greater than 20m (21.87 yards) and where the where the `angle` to the goal is also greater than 35 degrees. 33 shots were replaced in the Open Play dataset (2.71% of all goals).\n",
    "\n",
    "This leaves a remaining 10,165 shots for analysis, corresponding to 1,142 goals (11.2%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.2'>4.2. Read in CSV as pandas DataFrame</a>\n",
    "The following cell read the the `CSV` file as a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data directory\n",
    "print(glob.glob(os.path.join(data_dir_shots, 'engineered/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in engineered Shots CSVs as a pandas DataFrames\n",
    "df_shots = pd.read_csv(os.path.join(data_dir_shots, 'engineered', 'complete_shots_engineered.csv'))\n",
    "#df_shots_train = pd.read_csv(os.path.join(data_dir_shots, 'engineered', 'train_shots_engineered.csv'))\n",
    "#df_shots_test = pd.read_csv(os.path.join(data_dir_shots, 'engineered', 'test_shots_engineered.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.3'>4.3. Initial Data Handling</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.3.1'>4.3.1. Summary Report</a>\n",
    "Initial step of the data handling and Exploratory Data Analysis (EDA) is to create a quick summary report of the dataset using [pandas Profiling Report](https://github.com/pandas-profiling/pandas-profiling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data using pandas Profiling Report\n",
    "pp.ProfileReport(df_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#sectio4.3.2'>4.3.2. Further Inspection</a>\n",
    "The following commands go into more bespoke summary of the dataset. Some of the commands include content covered in the [pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) summary above, but using the standard [pandas](https://pandas.pydata.org/) functions and methods that most peoplem will be more familiar with.\n",
    "\n",
    "First check the quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the engineered DataFrame, df_shots\n",
    "df_shots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the engineered DataFrame, df_shots\n",
    "df_shots.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the engineered DataFrame, df_shots\n",
    "print(df_shots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the engineered DataFrame, df_shots\n",
    "print(df_shots.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has thirty features (columns). Full details of these attributes can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of the features of the engineered DataFrame, df_shots\n",
    "df_shots.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full details of these attributes and their data types can be found in the [Data Dictionary](#section6.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statements about the dataset\n",
    "\n",
    "## Assign variables\n",
    "count_shots = len(df_shots)\n",
    "count_goals = len(df_shots[(df_shots['outcome'] == 'Goal') | (df_shots['outcome'] == 'owngoal')])\n",
    "cols = list(df_shots)\n",
    "vals_play_type = df_shots['play_type'].unique()\n",
    "vals_body_part = df_shots['BodyPart'].unique()\n",
    "vals_interference = df_shots['Interference_on_Shooter'].unique()\n",
    "vals_outcome = df_shots['outcome'].unique()\n",
    "\n",
    "## Print statements\n",
    "print(f'The engineered shots DataFrame contains {count_shots:,} shots and {count_goals:,} goals ({round(100*count_goals/count_shots,1)}%).\\n')\n",
    "print(f\"The dataset contains the following columns: {cols}\\n\")\n",
    "print(f\"Unique values in the 'play_type' column: {vals_play_type}\\n\")    \n",
    "print(f\"Unique values in the 'BodyPart' column: {vals_body_part}\\n\")    \n",
    "print(f\"Unique values in the 'Interference_on_Shooter' column: {vals_interference}\\n\")    \n",
    "print(f\"Unique values in the 'outcome' column: {vals_outcome}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shot outcomes types and their frequency - 'Goal' count inaccurate as some outlier goals where changed using the 'isGoal' attribute (see following command)\n",
    "df_shots.groupby(['outcome']).outcome.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Goal' count inaccurate as some outlier goals where changed via the `isGoal` attribute from 1 to 0 (see following command):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shot outcomes types and their frequency\n",
    "df_shots.groupby(['isGoal']).outcome.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accurate Goal count - 1,142 goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shot interference types and their frequency\n",
    "df_shots.groupby(['Interference_on_Shooter']).outcome.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_shots\n",
    "df_shots.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the DataFrame, df_shots, showing some summary statistics for each numerical column in the DataFrame\n",
    "df_shots.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the engineered DataFrame, df_shots\n",
    "msno.matrix(df_shots, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_shots.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us very quickly that there are no missing values as they were treated in the first notebook, which is good as unlike [XGBoost](https://xgboost.readthedocs.io/en/latest/), [CatBoost](https://catboost.ai/) does not handle NULL values.\n",
    "\n",
    "If the dataset did contain NULL values, these would be required to be filled, which can be done easily using the following code::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NULL values with placeholders - 999\n",
    "#df_shots.fillna(-999, inplace=True)\n",
    "#null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section5'>5. Data Engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.1'>5.1. Create `head_foot` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary of BodyPart codes\n",
    "dict_head_foot = {'Left': 'Foot',\n",
    "                  'Right': 'Foot',\n",
    "                  'Head': 'Head',\n",
    "                 }\n",
    "\n",
    "# Map BodyPartCode to DataFrame\n",
    "df_shots['head_foot'] = df_shots['BodyPart'].map(dict_head_foot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section6'>6. Initial Modeling</a>\n",
    "First model created as a baseline for which interations of improvement are based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.1'>6.1. Declare Feature Vector and Target Variable</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features, as determined in the final Logistic Regression notebook\n",
    "features = ['distance_to_goalM',\n",
    "            'angle',\n",
    "            'Number_Intervening_Opponents',\n",
    "            'Number_Intervening_Teammates',\n",
    "            'head_foot',\n",
    "            'Interference_on_Shooter',\n",
    "            'header_distance_to_goalM'\n",
    "           ]\n",
    "\n",
    "\"\"\"\n",
    "# Alternative feature list: features prepared in the previous notebook that one-hot encoded categorical features, however, CatBoost can deal with categorical features\n",
    "features = ['distance_to_goalM',\n",
    "            'angle',\n",
    "            'Number_Intervening_Opponents',\n",
    "            'Number_Intervening_Teammates',\n",
    "            'isFoot',\n",
    "            'High',\n",
    "            'Low',\n",
    "            'header_distance_to_goalM'\n",
    "           ]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X = df_shots[features]\n",
    "y = df_shots['isGoal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at feature vector (`X`) and target variable (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature vector\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target variable\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.2'>6.2. State the Categorical Features\n",
    "Requirement for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical variables\n",
    "cat_features = ['Number_Intervening_Opponents',\n",
    "                'Number_Intervening_Teammates',\n",
    "                'head_foot',\n",
    "                'Interference_on_Shooter',\n",
    "               ]\n",
    "\n",
    "# Alternative method - all variables that aren't floats\n",
    "#cat_features = np.where(X_train[features].dtypes != np.float)[0]\n",
    "\n",
    "# See categories\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.3'>6.3. Look at the Label Balance of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Labels: {set(y)}')\n",
    "print(f'Zero count = {len(y) - sum(y)}, One count = {sum(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is unbalanced. This observation is considered as part of the decision making for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.4'>6.4. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign data to be used\n",
    "df = X.corr()\n",
    "top_corr_features = df.index\n",
    "\n",
    "## Set background colour\n",
    "background = 'aliceblue'\n",
    "\n",
    "## Create figure \n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig.set_facecolor(background)\n",
    "\n",
    "## Seaborn heat map\n",
    "sns.heatmap(df , \n",
    "            xticklabels=df.columns,\n",
    "            yticklabels=df.columns,\n",
    "            annot=True,\n",
    "            cmap='RdYlGn',\n",
    "           #vmin=0,\n",
    "           #vmax=0.5\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.5'>6.5. Store Dataset as a Pool Class\n",
    "There are several ways of passing dataset to training - using X, y or using Pool class. Pool class is the class for storing the dataset (see the next few commands).\n",
    "\n",
    "You can use Pool class if the dataset has more than just X and y (for example, it has sample weights or groups) or if the dataset is large and it takes long time to read it into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(data=X, label=y, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.6'>6.6. Split Data into Separate Training and Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into Training and Testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train pool\n",
    "train_pool = Pool(data=X_train, \n",
    "                  label=y_train, \n",
    "                  cat_features=cat_features\n",
    "                 )\n",
    "\n",
    "# Define Test pool\n",
    "test_pool = Pool(data=X_test, \n",
    "                 label=y_test, \n",
    "                 cat_features=cat_features\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.7'>6.7. Train the Baseline CatBoost\n",
    "When selection the objective function, for binary classification there are two options:\n",
    "1.    `Logloss` for binary target.\n",
    "2.    `CrossEntropy` for probabilities in target.    \n",
    "\n",
    "All available evaluations metrics: [eval_metric](https://catboost.ai/docs/concepts/python-reference_utils_eval_metric.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare initial parameters - will later be tuned\n",
    "params = {'iterations': 1_000,\n",
    "          'learning_rate': 0.5,                   # default learning_rate for binary classification\n",
    "          'cat_features': cat_features,           # unique to CatBoost, parameter to state the categorical features\n",
    "          'depth': 2,\n",
    "         #'eval_metric': 'Logloss',               # 'AUC',\n",
    "         #'custom_metric': ['Logloss', 'AUC'],\n",
    "          'custom_loss': ['Logloss', 'AUC'],\n",
    "          'verbose': 10,    # True\n",
    "         }\n",
    "\n",
    "# Display parameters\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the classifier \n",
    "catboost_clf = CatBoostClassifier(**params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "catboost_clf.fit(X_train,\n",
    "                 y_train,\n",
    "                 eval_set=(X_test, y_test),\n",
    "                 use_best_model=True,\n",
    "                 plot=True\n",
    "                )\n",
    "\n",
    "# Print statements\n",
    "print(f'Model is fitted: {catboost_clf.is_fitted()}')\n",
    "print(f'Model params:\\n{catboost_clf.get_params()}')\n",
    "print(f'Tree count: {catboost_clf.tree_count_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other way to visualise\n",
    "#MetricVisualizer(['catboost_clf']).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.7'>5.7. Predictions on Test Data and Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities of Test set\n",
    "y_pred = catboost_clf.predict_proba(X_test)\n",
    "\n",
    "print(f'Log Loss of the initial CatBoost model: {sk_metrics.log_loss(y_test, y_pred):.5f}')\n",
    "#print(f'AUC of the initial CatBoost model: {sk_metrics.roc_auc_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Probability Predictions array to a pandas DataFrame\n",
    "df_predictions_initial = pd.DataFrame(y_pred, columns = ['prob_no_goal', 'prob_goal'])\n",
    "\n",
    "# Join the Probability Predictions back onto the original Test DataFrame\n",
    "df_test_predictions_initial = pd.merge(X_test, df_predictions_initial, left_index=True, right_index=True)\n",
    "\n",
    "# Display DataFrame\n",
    "df_test_predictions_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.8'>5.8. Feature Importance\n",
    "The features will be analysed in more detail using the [SHAP](https://shap.readthedocs.io/en/latest/) later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_import = [t for t in zip(features, catboost_clf.get_feature_importance())]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns=['Feature', 'VarImp'])\n",
    "feat_import_df = feat_import_df.sort_values('VarImp', ascending=False)\n",
    "feat_import_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.9'>5.9. Visualise Initial CatBoost Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.10'>5.10. Evaluation\n",
    "We have the starting [CatBoost](https://catboost.ai/) model, which is performing worse than the final Logistic Regression model, with a Log Loss on test set of 0.29954. To beat the performance of the existing Logistic Regression model, we're looking to reduce this Log Loss to ~0.280. Improvement of the Log Loss is done through k-fold Cross Validation and Hyperparameter Optimisation, conducted in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section6'>6. Cross-validation\n",
    "Documentation [[link](https://catboost.ai/docs/concepts/python-reference_cv.html)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare initial parameters - will later be tuned\n",
    "params = {'iterations': 1_000,\n",
    "          'learning_rate': 0.5,\n",
    "          'cat_features': cat_features,\n",
    "          'depth': 2,\n",
    "          'loss_function': 'Logloss',\n",
    "          'custom_loss': 'AUC',\n",
    "          'verbose': 10    # True\n",
    "         }\n",
    "\n",
    "# Display parameters\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_data = cv(params=params,\n",
    "             pool=train_pool,\n",
    "             fold_count=10,\n",
    "             shuffle=True,\n",
    "             stratified=True,    # stratified by default, important for an unbalanced dataset\n",
    "             partition_random_seed=42,\n",
    "             plot=True,\n",
    "             verbose=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_value = np.min(cv_data['test-Logloss-mean'])\n",
    "standard_dev = cv_data['test-Logloss-std'][best_iter]\n",
    "best_iter = np.argmin(cv_data['test-Logloss-mean'])\n",
    "\n",
    "print(f'Best validation Log Loss score, not stratified: {best_value:.4f}±{standard_dev:.4f} on step {best_iter}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section7'>7. Feature Importance\n",
    "The features will be analysed in more detail using the [SHAP](https://shap.readthedocs.io/en/latest/) later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = [t for t in zip(features, catboost_clf.get_feature_importance())]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns=['Feature', 'VarImp'])\n",
    "feat_import_df = feat_import_df.sort_values('VarImp', ascending=False)\n",
    "feat_import_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    We can see that the feature `angle` has been given the highest importance score among all the features, closely followed by `distance_to_goalM`.\n",
    "*    Based upon this importance score, we can select the features with highest importance score and discard the redundant ones.\n",
    "*    Thus CatBoost also gives us a way to do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section8'>8. Hyperparameter Optimisation\n",
    "Currently developed a simple baseline CatBoost model.\n",
    "    \n",
    "The next step is to tune these hyperparameters to improve the model and take full advantage of the CatBoost library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section8.1'>8.1. Grid Search\n",
    "Documentation [[link](https://catboost.ai/docs/concepts/python-reference_catboost_grid_search.html)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_clf = CatBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'learning_rate': [0.03, 0.1],\n",
    "        'depth': [4, 6, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_result = catboost_clf.grid_search(grid, \n",
    "                                              X=train_pool, \n",
    "                                              y=None,\n",
    "                                              cv=10,\n",
    "                                              partition_random_seed=42,\n",
    "                                              calc_cv_statistics=True,\n",
    "                                              search_by_train_test_split=True,\n",
    "                                              refit=True,\n",
    "                                              shuffle=True,\n",
    "                                              stratified=True,\n",
    "                                              verbose=True,\n",
    "                                              plot=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters giving the best value of the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available cross-validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result['cv_results'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality estimated using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result['cv_results']['test-Logloss-mean'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is ready to use after searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = catboost_clf.predict_proba(test_pool)\n",
    "predicted[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section8.2'>8.2. Randomised Search\n",
    "Documentation [[link](https://catboost.ai/docs/concepts/python-reference_catboost_randomized_search.html)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare initial parameters - will later be tuned\n",
    "tuned_params = {'iterations': 1_000,\n",
    "                'learning_rate': 0.1,\n",
    "                'l2_leaf_reg': 9,\n",
    "                'cat_features': cat_features,\n",
    "                'depth': 4,\n",
    "                'loss_function': 'Logloss',\n",
    "                'custom_loss': 'AUC',\n",
    "                'verbose': 10    # True\n",
    "               }\n",
    "\n",
    "# Display parameters\n",
    "tuned_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section9'>9. Overfitting Detector</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_early_stop = CatBoostClassifier(eval_metric='Logloss',    # Logloss by default\n",
    "                                           iterations=1_000,\n",
    "                                           learning_rate=0.1,\n",
    "                                           early_stopping_rounds=20\n",
    "                                          )\n",
    "\n",
    "model_with_early_stop.fit(train_pool,\n",
    "                          eval_set=test_pool,\n",
    "                          verbose=True,\n",
    "                          plot=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_with_early_stop.tree_count_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section10'>10. Final Optimised CatBoost Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section11'>11. Performance Comparison of CatBoost with XGBoost and Logistic Regression</a>\n",
    "\n",
    "Final reported Log Loss for each model:\n",
    "*    CatBoost: ....\n",
    "*    XGBoost: 0.28600\n",
    "*    Logistic Regression: 0.28924\n",
    "\n",
    "This is a reduction of 0.00324 when using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section12'>12. Assessment of the Performance of the Teams in Game 2 of the Metrica Sports Shot Data\n",
    "The next stage of this analysis to take the Chance Quality Model derived and apply it to the DataFrame of identified shots from game 2 of the sample Metrica Sports data. to determine which team deserved to win the game, based solely on the quality of chances that each team created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section14.1'>14.1. Apply the Trained Chance Quality Model from XGBoost to the Metrica Sports Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Metrica Sports Shots data for game 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data directory\n",
    "print(glob.glob(os.path.join(data_dir_metrica, 'engineered/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in exported Metrica Sports game 2 shots CSV as a pandas DataFrame\n",
    "df_metrica = pd.read_csv(os.path.join(data_dir_metrica, 'engineered', 'game_2_shots_with_xg.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed video of the 24 shots in game 2 of the Metrica Sports sample data\n",
    "Video('../../video/fig/metrica-sports/tracking_shots_all.mp4', width=770, height=530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename xG column derived in previous model using Logistic Regression (later compared to value derived from XGBoost) and drop previous prediction columns\n",
    "df_metrica = (df_metrica\n",
    "                  .rename(columns={'xG': 'xG_LR'})\n",
    "                  .drop(['prob_no_goal', 'prob_goal'], axis=1)\n",
    "             )\n",
    "\n",
    "# Display DataFrame\n",
    "df_metrica.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subset Metrica Sports data to be compatible with trained Chance Quality Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to transform both the Chance Quality model and the Metrica Sports data to be compatible with trained Chance Quality Model\n",
    "def transformation_metrica(df):\n",
    "    \"\"\"\n",
    "    Function performs all transformation steps that took place in the first CQM notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Compute the distance to the center - distance_to_centerM\n",
    "    df['distance_to_centerM'] = np.abs(df['position_yM'])\n",
    "\n",
    "    # Compute header distance to goal\n",
    "    df['header_distance_to_goalM'] = df['isHead'] * df['distance_to_goalM']\n",
    "    \n",
    "    # One-hot encode the Interference on the Shooter\n",
    "    \n",
    "    ## One-hot encoding\n",
    "    df_dummy_interference_on_shooter = pd.get_dummies(df['Interference_on_Shooter'])\n",
    "    \n",
    "    # Attach separate columns back onto the dataset\n",
    "    df = pd.concat([df, df_dummy_interference_on_shooter], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and subset Metrica Sports data to be compatible with trained Chance Quality Model\n",
    "\n",
    "## Transform data\n",
    "df_metrica_trans = transformation_metrica(df_metrica)\n",
    "\n",
    "## Define Features\n",
    "features = ['distance_to_goalM',\n",
    "            'angle',\n",
    "            'Number_Intervening_Opponents',\n",
    "            'Number_Intervening_Teammates',\n",
    "            'isFoot',\n",
    "            'High',     # 'Interference_on_Shooter'\n",
    "            'Low',       # 'Interference_on_Shooter'\n",
    "            'header_distance_to_goalM'\n",
    "           ]\n",
    "\n",
    "## Transform Metrica Sports data \n",
    "df_metrica_trans = df_metrica_trans[features]\n",
    "\n",
    "# Display DataFrame\n",
    "df_metrica_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities of MetricaSports shot data\n",
    "y_pred_metrica = xgb_clf_final.predict_proba(df_metrica_trans[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Probability Predictions array to a pandas DataFrame\n",
    "df_metrica_predictions = pd.DataFrame(y_pred_metrica, columns=['prob_no_goal', 'prob_goal'])\n",
    "\n",
    "# Join the Probability Predictions back onto the original test DataFrame\n",
    "df_metrica_predictions_final = pd.merge(df_metrica, df_metrica_predictions, left_index=True, right_index=True)\n",
    "\n",
    "# Create xG column and assign all penalties not accounted for in trained model an xG of 0.75\n",
    "df_metrica_xg = df_metrica_predictions_final\n",
    "df_metrica_xg['xG'] = df_metrica_xg['prob_goal']\n",
    "df_metrica_xg['xG'] = np.where(df_metrica_xg['isPenalty'] == 1, 0.76, df_metrica_xg['xG'])\n",
    "\n",
    "# Display DataFrame\n",
    "df_metrica_xg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section14.2'>14.2. Assessment of the Performance of the Teams in Game 2 of the Metrica Sports Shot Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xG Race Chart of XGBoost model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xG Race Chart of XGBoost model with revised xG for penalties\n",
    "\n",
    "## Assign DataFrame\n",
    "df = df_metrica_xg\n",
    "\n",
    "## Create four lists to plot the different xG values - home, away, xG, and minutes. We start these with zero so our charts will start at 0\n",
    "a_xG = [0]\n",
    "h_xG = [0]\n",
    "a_min = [0]\n",
    "h_min = [0]\n",
    "\n",
    "## Define team names from the DataFrame\n",
    "hteam = 'Home'\n",
    "ateam = 'Away'\n",
    "\n",
    "## For loop to append the xG and minute for both the Home and Away teams\n",
    "for x in range(len(df['xG'])):\n",
    "    if df['team'][x]==ateam:\n",
    "        a_xG.append(df['xG'][x])\n",
    "        a_min.append(df['match_minute'][x])\n",
    "    if df['team'][x]==hteam:\n",
    "        h_xG.append(df['xG'][x])\n",
    "        h_min.append(df['match_minute'][x])\n",
    "        \n",
    "## Function we use to make the xG values be cumulative rather than single shot values. Foes through the list and adds the numbers together\n",
    "def nums_cumulative_sum(nums_list):\n",
    "    return [sum(nums_list[:i+1]) for i in range(len(nums_list))]\n",
    "\n",
    "## Apply defned nums_cumulative_sum function to the home and away xG lists\n",
    "a_cumulative = nums_cumulative_sum(a_xG)\n",
    "h_cumulative = nums_cumulative_sum(h_xG)\n",
    "\n",
    "## Find the total xG. Create a new variable from the last item in the cumulative list\n",
    "alast = round(a_cumulative[-1],2)\n",
    "hlast = round(h_cumulative[-1],2)\n",
    "\n",
    "## Set background colour\n",
    "background = 'aliceblue'\n",
    "\n",
    "## Create figure \n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "fig.set_facecolor(background)\n",
    "ax.patch.set_facecolor(background)\n",
    "\n",
    "# Set up our base layer\n",
    "mpl.rcParams['xtick.color'] = 'midnightblue'\n",
    "mpl.rcParams['ytick.color'] = 'midnightblue'\n",
    "\n",
    "## Create xG Race Chart\n",
    "plt.xticks([0, 15, 30, 45, 60, 75, 90])\n",
    "plt.xlabel('Minute', fontfamily='DejaVu Sans', color='midnightblue', fontsize=16)\n",
    "plt.ylabel('xG', fontfamily='DejaVu Sans', color='midnightblue', fontsize=16)\n",
    "\n",
    "# Plot the step graphs\n",
    "ax.step(x=a_min, y=a_cumulative, color='blue', label=ateam, linewidth=5, where='post')\n",
    "ax.step(x=h_min, y=h_cumulative, color='red', label=ateam, linewidth=5, where='post')\n",
    "\n",
    "## Set Gridlines \n",
    "ax.grid(linewidth=0.25, color='midnightblue', axis='y', zorder=1)\n",
    "spines = ['top','bottom','left','right']\n",
    "for x in spines:\n",
    "    if x in spines:\n",
    "        ax.spines[x].set_visible(False)\n",
    "\n",
    "## Set title\n",
    "ax.set_title('xG Race Chart (XGBoost)',\n",
    "             loc='center',\n",
    "             color='midnightblue', \n",
    "             fontweight='bold',\n",
    "             fontfamily='DejaVu Sans',\n",
    "             fontsize=22,\n",
    "            )\n",
    "\n",
    "## Show Legend\n",
    "#plt.legend()\n",
    "\n",
    "## Save figure\n",
    "if not os.path.exists(fig_shots_dir + '/metrica_shots_race_chart_xg_xgboost.png'):\n",
    "    plt.savefig(fig_shots_dir + '/metrica_shots_race_chart_xg_xgboost.png', bbox_inches='tight', dpi=300)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "## Show figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xG Race Chart of Logistic Regression model predictions (derived in the previous CQM notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create xG Race Chart of Logistic Regression model with revised xG for penalties\n",
    "\n",
    "## Assign DataFrame\n",
    "df = df_metrica_xg\n",
    "\n",
    "## Create four lists to plot the different xG values - home, away, xG, and minutes. We start these with zero so our charts will start at 0\n",
    "a_xG = [0]\n",
    "h_xG = [0]\n",
    "a_min = [0]\n",
    "h_min = [0]\n",
    "\n",
    "## Define team names from the DataFrame\n",
    "hteam = 'Home'\n",
    "ateam = 'Away'\n",
    "\n",
    "## For loop to append the xG and minute for both the Home and Away teams\n",
    "for x in range(len(df['xG_LR'])):\n",
    "    if df['team'][x]==ateam:\n",
    "        a_xG.append(df['xG_LR'][x])\n",
    "        a_min.append(df['match_minute'][x])\n",
    "    if df['team'][x]==hteam:\n",
    "        h_xG.append(df['xG_LR'][x])\n",
    "        h_min.append(df['match_minute'][x])\n",
    "        \n",
    "## Function we use to make the xG values be cumulative rather than single shot values. Foes through the list and adds the numbers together\n",
    "def nums_cumulative_sum(nums_list):\n",
    "    return [sum(nums_list[:i+1]) for i in range(len(nums_list))]\n",
    "\n",
    "## Apply defned nums_cumulative_sum function to the home and away xG lists\n",
    "a_cumulative = nums_cumulative_sum(a_xG)\n",
    "h_cumulative = nums_cumulative_sum(h_xG)\n",
    "\n",
    "## Find the total xG. Create a new variable from the last item in the cumulative list\n",
    "alast = round(a_cumulative[-1],2)\n",
    "hlast = round(h_cumulative[-1],2)\n",
    "\n",
    "## Set background colour\n",
    "background = 'aliceblue'\n",
    "\n",
    "## Create figure \n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "fig.set_facecolor(background)\n",
    "ax.patch.set_facecolor(background)\n",
    "\n",
    "# Set up our base layer\n",
    "mpl.rcParams['xtick.color'] = 'midnightblue'\n",
    "mpl.rcParams['ytick.color'] = 'midnightblue'\n",
    "\n",
    "## Create xG Race Chart\n",
    "plt.xticks([0, 15, 30, 45, 60, 75, 90])\n",
    "plt.xlabel('Minute', fontfamily='DejaVu Sans', color='midnightblue', fontsize=16)\n",
    "plt.ylabel('xG', fontfamily='DejaVu Sans', color='midnightblue', fontsize=16)\n",
    "\n",
    "# Plot the step graphs\n",
    "ax.step(x=a_min, y=a_cumulative, color='blue', label=ateam, linewidth=5, where='post')\n",
    "ax.step(x=h_min, y=h_cumulative, color='red', label=ateam, linewidth=5, where='post')\n",
    "\n",
    "## Set Gridlines \n",
    "ax.grid(linewidth=0.25, color='midnightblue', axis='y', zorder=1)\n",
    "spines = ['top','bottom','left','right']\n",
    "for x in spines:\n",
    "    if x in spines:\n",
    "        ax.spines[x].set_visible(False)\n",
    "\n",
    "## Set title\n",
    "ax.set_title('xG Race Chart (Logistic Regression)',\n",
    "             loc='center',\n",
    "             color='midnightblue', \n",
    "             fontweight='bold',\n",
    "             fontfamily='DejaVu Sans',\n",
    "             fontsize=22,\n",
    "            )\n",
    "\n",
    "## Show Legend\n",
    "#plt.legend()\n",
    "\n",
    "## Save figure\n",
    "if not os.path.exists(fig_shots_dir + '/metrica_shots_race_chart_xg_lr.png'):\n",
    "    plt.savefig(fig_shots_dir + '/metrica_shots_race_chart_xg_lr.png', bbox_inches='tight', dpi=300)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "## Show figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the xG Race Charts for both the XGBoost and Logistic Regression Chance Quality Models both predict that, the Away team (blue) is the team that accumlates the greatest amount of xG during the course of the 90 minutes. However, in the XGBoost model, the gap is much narrower, further confirming that these basic models need to be tested with more data and analysed more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section14.3'>14.3. Comparison of the XGBoost and Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename xG column derived from XGBoost (used to compared to previous calculated xG value in Logistic Regression model) and create a variance column\n",
    "df_metrica_xg = df_metrica_xg.rename(columns={'xG': 'xG_XGB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two xG columns of the LR and XGB models\n",
    "\n",
    "## Difference in xG between LR and XGB models\n",
    "df_metrica_xg['diff_xG'] = df_metrica_xg['xG_LR'] - df_metrica_xg['xG_XGB']\n",
    "\n",
    "## Percentage variance in xG between LR and XGB models\n",
    "df_metrica_xg['percentage_variance_xG'] = (((df_metrica_xg['xG_XGB'] - df_metrica_xg['xG_LR']) / df_metrica_xg['xG_LR'])*100).round(2)\n",
    "\n",
    "# Display DataFrame\n",
    "df_metrica_xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the selected columns\n",
    "df_metrica_xg[['xG_LR', 'xG_XGB', 'diff_xG', 'percentage_variance_xG']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `diff_xG` and `percentage_variance_xG` show the difference between the predictions made by the Logistic Regression and XGBoost models. 8 of the 24 shots have a variance greater than 50%. I won't go into too much detail here about these variances at this stage (I might come back to this later on), but to summarise, these simple models show that much more testing that is required before the predictions can be confirmed as robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section14.4'>14.4. Export the Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final dataset\n",
    "df_metrica_xg.to_csv(os.path.join(data_dir_metrica, 'engineered', 'game_2_shots_with_xg_lr_xgb.csv'), index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section15'>15. Summary</a>\n",
    "To summarise, this notebook builds upon the initial Chance Quality Model created using Logistic Regression from shots data, this time using XGBoost. This notebook then compares the variances between the two models\n",
    "\n",
    "The steps to create this model can be summarised as the following:\n",
    "1.     Set up the notebook for an environment in which to apply the XGBoost know Machine Learning algorithm to a dataset of shots data.\n",
    "2.    Re-explained the challenge to create a Chance Quality Model and defined the key proxy in which to determine this - <b>Expected Goals</b>.\n",
    "3.     Imported the provided CSV data file imported as a pandas DataFrame and conducted a basic Exploratory Data Analysis.\n",
    "4.     Introduction to XGBoost.\n",
    "5.     Explanation of the difference between Bagging Vs. Boosting.\n",
    "6.     Theory Behind the XGBoost Algorithm.\n",
    "7.     Imported the engineered shots dataset created in the first Chance Quality Model notebook.\n",
    "8.     Created an initial model using the data and features that were immediately available from the starting data, before any model optimisation, to determine a baseline figure for the model. This data was split into a training and a test set, which were kept separate during the entire modelling process and the test data was never incorporated into the training data.\n",
    "9.     k-fold Cross Validation using XGBoost.\n",
    "10.    Feature Importance with XGBoost.\n",
    "11.    Visualisation of XGBoost Tree.\n",
    "12.    Hyperparameter Optimisation.\n",
    "13.    Final Optimised XGBoost Model - training of the final model using the parameters found via Grid Search.\n",
    "14.    Performance comparison of XGBoost with Logistic Regression - final XGBoost model has a resulting Log Loss of 0.28600, where the final Logistic Regression had a Log Loss of 0.28924 - reduction of 0.00324.\n",
    "15.    Assessment of the teams performance in the Metrica Sports data to determine who deserved to win the game, based solely on the quality of chances that each team created - the <b>Away</b> (blue) team. The predicted xG values for the shots determined by the two Chance Quality Models were compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section16'>16. Next Steps</a>\n",
    "\n",
    "Proposals and ideas for further work can be divided into two sections - Expected Goals Models and Tracking data. Current suggestions are as follows:\n",
    "\n",
    "#### Expected Goals Models\n",
    "*    The focus of my approach to answer this data challenge was to not build the absolute best performing ML model, with the best performance metrics and fanciest algorithm. My objective was to conduct and end-to-end process for building a model, including all the key stages such as feature engineering, univariate and multivariate analysis, and iterated performance assessment and improvement – for this reason, for the submission of the data task, a simple Chance Quality Model was built using Logistic Regression. However, Gradient Boosting algorithms lead to improved performance and for this reason, this second notebook (of currently two) creates a Chance Quality Model using [XGBoost](https://xgboost.readthedocs.io/en/latest/). Potential further models that can be deployed to try and further improve the performance of the Chance Quality model include other Gradient Boosting algorithms, such as [LightGBM](https://lightgbm.readthedocs.io/en/latest/) or [Catboost](https://catboost.ai/). More detail about these modeling approaches can be found in my the Data Science pack that I submitted as part of my initial application (see: https://docs.google.com/presentation/d/16stYbJoI8aYqtn_grJHSdTbMA1xwo-Iy9g9JJruMOBQ/edit?usp=sharing). \n",
    "\n",
    "*    Application of a full Event data set, such as those from StatsBomb and Wyscout, to create an Expected Goals model with more features. Such features that were not possible to include in this model but that could be added with Event and/or Tracking data include: strong/weak foot, flag for counter attack, flag for smart pass, determine whether a shot had been immediately taken before, whether the shot was from a cross. This is discussed in more detail in the Feature Engineering section (section 9) of the first Chance Quality Model notebook [[link](https://nbviewer.jupyter.org/github/eddwebster/mcfc_submission/blob/main/notebooks/chance_quality_modelling/Creating%20a%20Chance%20Quality%20Model%20from%20Shots%20Data.ipynb)]. A comparison of the features used in the respected xG models by [Sam Green](https://twitter.com/aSamGreen) and [Michael Caley](https://twitter.com/MC_of_A) can be found in the index.\n",
    "\n",
    "*    To see really test the performance of this model, it would be great to quality test performance and xG prediction with those of other providers such as StatsBomb and observe the level of variance in predictions between this basic model and a professional one created using a much larger dataset with much more features.\n",
    "\n",
    "*    Creation of separate Expected Goals models for Direct Free Kicks and Corners. Currently, only Open-Play shots considered and a xG value for penalties was taken from StatsBomb/FBref [[link](https://fbref.com/en/expected-goals-model-explained/)]. \n",
    "\n",
    "*    Add fake shots to the shots data – see David Sumpter’s tweet for the benefits of including fake data in an Expected Goals model [[link](https://twitter.com/Soccermatics/status/1260598182624575490)].\n",
    "\n",
    "#### Tracking data\n",
    "*    This current analysis only focuses on the shots taken by the teams. The next stage of this analysis would be to apply this Tracking data, not to just a Shots dataset, but to a full Events dataset, taking the basic concepts of analysis and feature extraction observed in this submission and then start to apply more sophisticated modeling approaches such as Pitch Control or Expected Possession Value (EPV) models such as the VAEP model by SciSports and KULeuven, or the Expected Threat (xT) model by Karun Singh. This can be taken on further, by combining these two modelling approaches to analyse value that certain actions of interest brought to the team during a particular play in the match and determine the Expected Value-Added. This was unfortunately not possible to do in this analysis as the Event data provided only included Shot data, but it would be something I would like to take on and do in the future, using publicly available Event data from StatsBomb and Wyscout, with the sample Tracking data from Metrica Sports. More detail about these models can be found in my the Data Science pack that I submitted in my initial application (see: https://docs.google.com/presentation/d/16stYbJoI8aYqtn_grJHSdTbMA1xwo-Iy9g9JJruMOBQ/edit?usp=sharing). \n",
    "\n",
    "*    Further enrich the Event data through Tracking data, adding further detail and specificity, which again can be used to further improve the Expected Goals model. This was observed in this analysis with addition of the Intervening and Interfering teammates and opponents. Features that were not considered in this analysis, include aspects such as the goalkeeper and defender positions in the moment of the shot e.g.: how much of the goal was covered by the goalkeeper? are the defenders in position? These are attributes that can be derived from the Tracking data to gain additional insight previously not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section17'>17. References and Further Reading</a>\n",
    "Please see my [`football_analytics`](https://github.com/eddwebster/football_analytics) repository for my attempt to create as concise a list of possible of publicly available resources published by the football analytics community.\n",
    "\n",
    "The follow resources are those that were specifically used to inform and create my submission for the CFG Junior Data Scientist Data Challenge, specifically focusing on Expected Goals and Tracking data. I have also included links to other topics related to the role such as the application of Reinforcement Learning in football. Credits to all those cited below.\n",
    "\n",
    "This list of is also available in the project GitHub repo [[link](https://github.com/eddwebster/mcfc_submission)].\n",
    "\n",
    "### Football Analytics\n",
    "\n",
    "#### Tutorials\n",
    "*    Friends of Tracking YouTube channel [[link](https://www.youtube.com/channel/UCUBFJYcag8j2rm_9HkrrA7w)] and Mathematical Modelling of Football course by Uppsala University [[link](https://uppsala.instructure.com/courses/28112)]. The GitHub repo with all code featured can be found at the following [[link](https://github.com/Friends-of-Tracking-Data-FoTD)]. Lectures of note include:\n",
    "     +    Laurie Shaw's Metrica Sports Tracking data series for #FoT - [Introduction](https://www.youtube.com/watch?v=8TrleFklEsE), [Measuring Physical Performance](https://www.youtube.com/watch?v=VX3T-4lB2o0), [Pitch Control modelling](https://www.youtube.com/watch?v=5X1cSehLg6s), and [Valuing Actions](https://www.youtube.com/watch?v=KXSLKwADXKI). See the following for code [[link](https://github.com/Friends-of-Tracking-Data-FoTD/LaurieOnTracking)];\n",
    "     +    David Sumpter's Expected Goals webinars for #FoT - [How to Build An Expected Goals Model 1: Data and Model](https://www.youtube.com/watch?v=bpjLyFyLlXs), [How to Build An Expected Goals Model 2: Statistical fitting](https://www.youtube.com/watch?v=wHOgINJ5g54), and [The Ultimate Guide to Expected Goals](https://www.youtube.com/watch?v=310_eW0hUqQ). See the following for code [3xGModel](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/3xGModel.py), [4LinearRegression](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/4LinearRegression.py), [5xGModelFit.py](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/5xGModelFit.py), and [6MeasuresOfFit](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/6MeasuresOfFit.py);\n",
    "     +    Peter McKeever's ['Good practice in data visualisation'](https://www.youtube.com/watch?v=md0pdsWtq_o) webinar for Friends of Tracking. See the following for code [[link](https://github.com/petermckeeverPerform/friends-of-tracking-viz-lecture)];\n",
    "*    [Soccer Analytics Handbook](https://github.com/devinpleuler/analytics-handbook) by [Devin Pleuler](https://twitter.com/devinpleuler). See tutorial notebooks (also available in Google Colab) that notably include: [3. Logistic Regression](https://github.com/devinpleuler/analytics-handbook/blob/master/notebooks/logistic_regression.ipynb), and [7. Data Visualization](https://github.com/devinpleuler/analytics-handbook/blob/master/notebooks/data_visualization.ipynb):\n",
    "*    [FC Python](https://twitter.com/fc_python) tutorials [[link](https://fcpython.com/)];\n",
    "*    DataViz, Python, and matplotlib tutorials by Peter McKeever [[link](http://petermckeever.com/)] - I think his website is currently in redevelopment, with many of the old tutorials not currently available (28/02/2021). Check out his revamped [How to Draw a Football Pitch](http://petermckeever.com/2020/10/how-to-draw-a-football-pitch/) tutorial;\n",
    "*    [McKay Johns YouTube channel](https://www.youtube.com/channel/UCmqincDKps3syxvD4hbODSg);\n",
    "*    [Tech how-to: build your own Expected Goals model](https://www.scisports.com/tech-how-to-build-your-own-expected-goals-model/) by [Jan Van Haaren](https://twitter.com/JanVanHaaren) and [SciSports](https://twitter.com/SciSportsNL).\n",
    "*    [Fitting your own football xG model](https://www.datofutbol.cl/xg-model/) by [Dato Fútbol](https://twitter.com/DatoFutbol_cl) (Ismael Gómez Schmidt). See GitHub repo [[link](https://github.com/Dato-Futbol/xg-model)];\n",
    "*    [Python for Fantasy Football series](http://www.fantasyfutopia.com/python-for-fantasy-football-introduction/) by [Fantasy Futopia](https://twitter.com/FantasyFutopia) ([Thomas Whelan](https://twitter.com/tom_whelan)).  See the following posts:\n",
    "     +    [Introduction to Machine Learning](http://www.fantasyfutopia.com/python-for-fantasy-football-introduction-to-machine-learning/)\n",
    "     +    [Addressing Class Imbalance in Machine Learning](http://www.fantasyfutopia.com/python-for-fantasy-football-addressing-class-imbalance-in-machine-learning/)\n",
    "     +    [Addressing Class Imbalance Part 2](http://www.fantasyfutopia.com/python-for-fantasy-football-addressing-class-imbalance-part-2/)\n",
    "     +    [Understanding Random Forests](http://www.fantasyfutopia.com/python-for-fantasy-football-understanding-random-forests/)\n",
    "     +    [Feature Engineering for Machine Learning](http://www.fantasyfutopia.com/python-for-fantasy-football-feature-engineering-for-machine-learning/)\n",
    "*    [Building an Expected Goals Model in Python](https://web.archive.org/web/20200301071559/http://petermckeever.com/2019/01/building-an-expected-goals-model-in-python/) by [Peter McKeever](https://twitter.com/petermckeever) (using WayBackMachine);\n",
    "*    [An xG Model for Everyone in 20 minutes (ish)](https://differentgame.wordpress.com/2017/04/29/an-xg-model-for-everyone-in-20-minutes-ish/ ) by [Football Fact Man](https://twitter.com/footballfactman) (Paul Riley).\n",
    "*    [How to Draw a Football Pitch](http://petermckeever.com/2020/10/how-to-draw-a-football-pitch/) by Peter McKeever\n",
    "*    [How To Create xG Flow Charts in Python](https://www.youtube.com/watch?v=bvoOOYMQkac) by [McKay Johns](https://twitter.com/mckayjohns). For code, see [[link](https://github.com/mckayjohns/Viz-Templates)]\n",
    "\n",
    "#### Libaries and GitHub Repos\n",
    "*    [`Friends-of-Tracking-Data-FoTD`](https://github.com/Friends-of-Tracking-Data-FoTD);\n",
    "*    [`SoccermaticsForPython`](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython) - repo by David Sumpter dedicated for people getting started with Python using the concepts derived from the book Soccermatics;\n",
    "*    [`LaurieOnTracking`](https://github.com/Friends-of-Tracking-Data-FoTD/LaurieOnTracking) by [Laurie Shaw](https://twitter.com/EightyFivePoint) - Python code for working with Metrica tracking data; and\n",
    "*    [`Expected Goals Thesis`](https://github.com/andrewRowlinson/expected-goals-thesis) by [Andrew Rowlinson](https://twitter.com/numberstorm). See both his thesis [[link](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/FOOTBALL%20SHOT%20QUALITY%20-%20Visualizing%20the%20Quality%20of%20Football%20Soccer%20Goals.pdf)] and the following notebooks:\n",
    "     +    [Explore Data Quality Overlap](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/00-explore-data-quality-overlap.ipynb);\n",
    "     +    [Expected Goals Model](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/01-expected-goals-model.ipynb);\n",
    "     +    [Expected Goals Calculate xG and Shap](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/02-expected-goals-calculate-xg-and-shap.ipynb);\n",
    "     +    [Visualise Models](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/03-visualize-models.ipynb);\n",
    "     +    [kernel Density Probability Scoring](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/04-kernel-density-probability-scoring.ipynb);\n",
    "     +    [Simulate Match Results from xG](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/05-simulate-match-results-from-xg.ipynb);\n",
    "     +    [Freeze Frame Examples](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/06-freeze_frame-example.ipynb);\n",
    "     +    [Red Zone Heatmap](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/07-red-zone-heatmap.ipynb);\n",
    "     +    [Shots Follow Poisson Distribution](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/08-shots_follow_poisson_distribution.ipynb); and\n",
    "     +    [Angle Features](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/notebooks/09_figure3_angle_features.ipynb).\n",
    "*    [`expected_goals_deep_dive`](https://github.com/andrewsimplebet/expected_goals_deep_dive) by [Andrew Puopolo](https://twitter.com/andrew_puopolo). See the following notebooks:\n",
    "     +    [Setting Our Data Up](https://github.com/andrewsimplebet/expected_goals_deep_dive/blob/master/0.%20Setting%20Our%20Data%20Up.ipynb)\n",
    "     +    [Random Forest Cross Validation And Hyperparameter Tuning](https://github.com/andrewsimplebet/expected_goals_deep_dive/blob/master/1.%20Random%20Forest%20Cross%20Validation%20And%20Hyperparameter%20Tuning.ipynb)\n",
    "     +    [Comparing Logistic Regression and Random Forest For Expected Goals](https://github.com/andrewsimplebet/expected_goals_deep_dive/blob/master/2.%20Basic%20Logistic%20Regression%20and%20Comparison%20To%20Random%20Forests.ipynb)\n",
    "     +    [Calibrating Expected Goals Models](https://github.com/andrewsimplebet/expected_goals_deep_dive/blob/master/3.%20Calibrating%20Expected%20Goals%20Models.ipynb)\n",
    "     +    [Sanity Checking Our Expected Goals Model and Final Thoughts](https://github.com/andrewsimplebet/expected_goals_deep_dive/blob/master/4.%20Sanity%20Checking%20Our%20Expected%20Goals%20Models%20And%20Final%20Thoughts.ipynb)\n",
    "*    [`soccer_analytics`](https://github.com/CleKraus/soccer_analytics) by [Kraus Clemens](https://twitter.com/CleKraus). See the following notebooks:\n",
    "     +    [Expected goal model with logistic regression](https://github.com/CleKraus/soccer_analytics/blob/master/notebooks/expected_goal_model_lr.ipynb)\n",
    "     +    [Challenges using gradient boosters](https://github.com/CleKraus/soccer_analytics/blob/master/notebooks/challenges_with_gradient_boosters.ipynb)\n",
    "*    [`xg-model`](https://github.com/Dato-Futbol/xg-model)] by [Dato Fútbol](https://twitter.com/DatoFutbol_cl) (Ismael Gómez Schmidt)\n",
    "*    [`soccer-xg`](https://pypi.org/project/soccer-xg/) by [Jesse Davis](https://twitter.com/jessejdavis1) and [Pieter Robberechts](https://twitter.com/p_robberechts) - a Python package for training and analyzing expected goals (xG) models in soccer (not used this this assignment but referenced here); and\n",
    "*    [`Google Research Football`](https://github.com/google-research/football). See the Kaggle Competition alongside Manchester City [[link](https://www.kaggle.com/c/google-football) (ended October 2020).\n",
    "\n",
    "#### Written Pieces\n",
    "For a full list of Expected Goals literature, see the following [[link](https://docs.google.com/document/d/1OY0dxqXIBgncj0UDgb97zOtczC-b6JUknPFWgD77ng4/edit)].\n",
    "\n",
    "##### Papers\n",
    "The following Shiny App from Lars Maurath is a great tool for looking up publications [[link](https://larsmaurath.shinyapps.io/soccer-analytics-library/)].\n",
    "*    [Routine Inspection: A Playbook for Corner Kicks](https://www.springerprofessional.de/en/routine-inspection-a-playbook-for-corner-kicks/18671052) (2020) by [Laurie Shaw](https://twitter.com/EightyFivePoint) and Sudarshan 'Suds' Gopaladesikan.  Accompanying talk - [2020 Harvard Sports Analytics Lab](https://www.youtube.com/watch?v=yfPC1O_g-I8)];\n",
    "*    [Dynamic Analysis of Team Strategy in Professional Football](https://static.capabiliaserver.com/frontend/clients/barca/wp_prod/wp-content/uploads/2020/01/56ce723e-barca-conference-paper-laurie-shaw.pdf) (2019) by [Laurie Shaw](https://twitter.com/EightyFivePoint) and [Mark Glickman](https://twitter.com/glicko). Accompanying talks - [NESSIS 2019](https://www.youtube.com/watch?v=VU4BOu6VfbU), [2020 Google Sports Analytics Meetup](https://www.youtube.com/watch?v=aQ9L6IkWI8U);\n",
    "*    [Football Shot Quality: Visualising the Quality of Soccer/Football Shots](https://github.com/andrewRowlinson/expected-goals-thesis/blob/master/FOOTBALL%20SHOT%20QUALITY%20-%20Visualizing%20the%20Quality%20of%20Football%20Soccer%20Goals.pdf) by [Andrew Rowlinson](https://twitter.com/numberstorm). See his GitHub repo for code [[link](https://github.com/andrewRowlinson/expected-goals-thesis)]; and\n",
    "*    [Game Plan: What AI can do for Football, and What Football can do for AI](https://arxiv.org/pdf/2011.09192.pdf) (2020) by Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, Will Spearman, Tim Waskett, and Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, and Demis Hassabis;\n",
    "*    [Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/pdf/1907.11180.pdf) (2020) by Karol Kurach, Anton Raichuk, Piotr Stańczyk, Michał Zając, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, Sylvain Gelly. See the GitHub repo [[link](https://github.com/google-research/football)];\n",
    "*    [A Framework for the Fine-Grained Evaluation of the Instantaneous Expected Value of Soccer Possessions](https://arxiv.org/abs/2011.09426) (2020) by Javier Fernández, Luke Bornn and Daniel Cervone;\n",
    "*    [Decomposing the Immeasurable Sport: A deep learning expected possession value framework for soccer](https://www.semanticscholar.org/paper/Decomposing-the-Immeasurable-Sport%3A-A-deep-learning-Fern%C3%A1ndez/fc78b144a531a8ffdf3216a677f3a65e70dad3c7) (2019) by [Javier Fernández](https://twitter.com/JaviOnData), [Bornn](https://twitter.com/LukeBornn), and [Dan Cervone](https://twitter.com/dcervone0). Accompanying talks - [SSAC19](https://www.youtube.com/watch?v=JIa7Td3YXxI), [StatsBomb conference](https://www.youtube.com/watch?v=nfPEEbKJbpM);\n",
    "*    [Ready Player Run: Off-ball run identification and classification](https://static.capabiliaserver.com/frontend/clients/barca/wp_prod/wp-content/uploads/2020/01/40ba07f4-ready-player-run-barcelona.pdf) (2020) by [Sam Gregory](https://twitter.com/GregorydSam);\n",
    "*    [Beyond Expected Goals](https://www.researchgate.net/profile/William_Spearman/publication/327139841_Beyond_Expected_Goals/links/5b7c3023a6fdcc5f8b5932f7/Beyond-Expected-Goals.pdf) (2018) by [Will Spearman](https://twitter.com/the_spearman);\n",
    "*    [Wide Open Spaces: A statistical technique for measuring space creation in professional soccer](https://www.researchgate.net/publication/324942294_Wide_Open_Spaces_A_statistical_technique_for_measuring_space_creation_in_professional_soccer) (2018) by [Javier Fernandez](https://twitter.com/JaviOnData) and [Luke Bornn](https://twitter.com/LukeBornn);\n",
    "*    [“The Leicester City Fairytale?”: Utilizing New Soccer Analytics Tools to Compare Performance in the 15/16 & 16/17 EPL Seasons (2017)](https://userpages.umbc.edu/~nroy/courses/fall2018/cmisr/papers/soccer_analytics.pdf) by Hector Ruiz, Paul Power, Xinyu Wei, and Patrick Lucey;\n",
    "*    [Not all passes are created equal: objectively measuring the risk and reward of passes in soccer from tracking data](http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1605.pdf) (2017) by Paul Power, Hector Ruiz, Xinyu Wei, and Patrick Lucey. See Paul Power's talk [[link](https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3097983.3098051&file=power_tracking_data.mp4&download=true)] (downloadable MP4), and the webpage [[link](https://dl.acm.org/doi/10.1145/3097983.3098051)];\n",
    "*    [“Quality vs Quantity”: Improved Shot Prediction in Soccer using Strategic Features from Spatiotemporal Data](https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20150308192147/Quality-vs-Quantity%E2%80%9D-Improved-Shot-Prediction-in-Soccer-using-Strategic-Features-from-Spatiotemporal-Data-Paper.pdf) (2015) by Patrick Lucey, Alina Bialkowski, Mathew Monfort, Peter Carr, and Iain Matthews; and\n",
    "*    [A Framework for Tactical Analysis and Individual Offensive Production Assessment in Soccer Using Markov Chains](http://nessis.org/nessis11/rudd.pdf) (2011) by [Sarah Rudd](https://twitter.com/srudd_ok). Accompanying NESSIS talk on Metacafe [[link](https://www.metacafe.com/watch/7337475/2011_nessis_talk_by_sarah_rudd/)].\n",
    "\n",
    "##### Blogs\n",
    "*    [Sam Green](https://twitter.com/aSamGreen)'s [xG model](https://www.optasportspro.com/news-analysis/assessing-the-performance-of-premier-league-goalscorers/);\n",
    "*    [Michael Caley](https://twitter.com/MC_of_A)'s [xG model](https://cartilagefreecaptain.sbnation.com/2014/9/11/6131661/premier-league-projections-2014#methoderology);\n",
    "*    [Using Data to Analyse Team Formations](https://eightyfivepoints.blogspot.com/2019/11/using-data-to-analyse-team-formations.html) by [Laurie Shaw](https://twitter.com/EightyFivePoint);\n",
    "*    [Structure in football: putting formations into context](https://eightyfivepoints.blogspot.com/2020/12/structure-in-football-putting.html) by [Laurie Shaw](https://twitter.com/EightyFivePoint);\n",
    "*    [xG explained](https://fbref.com/en/expected-goals-model-explained/) by [FBref](https://twitter.com/fbref);\n",
    "*    [What are expected Goals?](https://www.americansocceranalysis.com/explanation) by [American Soccer Analysis](https://twitter.com/AnalysisEvolved);\n",
    "*    [David Sumpter](https://twitter.com/Soccermatics)'s Expected Goals pieces:\n",
    "     +    [Should you write about real goals or expected goals? A guide for journalists](https://soccermatics.medium.com/should-you-write-about-real-goals-or-expected-goals-a-guide-for-journalists-2cf0c7ec6bb6);\n",
    "     +    [Football’s magical equation?](https://soccermatics.medium.com/footballs-magical-equation-bfe212ce7d4a)\n",
    "     +    [The Geometry of Shooting](https://soccermatics.medium.com/the-geometry-of-shooting-ae7a67fdf760).\n",
    "*    [Michael Caley](https://twitter.com/MC_of_A)'s Expected Goals pieces:\n",
    "     +    [Shot Matrix I: Shot Location and Expected Goals](https://cartilagefreecaptain.sbnation.com/2013/11/13/5098186/shot-matrix-i-shot-location-and-expected-goals)\n",
    "     +    [Let's talk about expected goals](https://cartilagefreecaptain.sbnation.com/2015/4/10/8381071/football-statistics-expected-goals-michael-caley-deadspin)\n",
    "*    [Jesse Davis](https://twitter.com/jessejdavis1) and [Pieter Robberechts](https://twitter.com/p_robberechts)' Expected Goals pieces for KU Leuven;\n",
    "     +    [How Data Avilability Affects the Ability to learn Good xG Models](https://dtai.cs.kuleuven.be/sports/blog/how-data-availability-affects-the-ability-to-learn-good-xg-models)\n",
    "     +    [Illustrating the Interplay between Features and Models in xG](https://dtai.cs.kuleuven.be/sports/blog/illustrating-the-interplay-between-features-and-models-in-xg)\n",
    "     +    [How Data Quality Affects xG](https://dtai.cs.kuleuven.be/sports/blog/how-data-quality-affects-xg)\n",
    "*    [Will Gürpinar-Morgan](https://twitter.com/WillTGM)'s Expected Goals pieces:\n",
    "     +    [Unexpected goals](https://2plus2equals11.com/2015/12/31/unexpected-goals/) on [2+2=11](https://2plus2equals11.com/);\n",
    "     +    [Great Expectations](https://2plus2equals11.com/2015/05/31/great-expectations/) on [2+2=11](https://2plus2equals11.com/);\n",
    "     +    [On single match expected goal totals](https://2plus2equals11.com/2015/12/16/on-single-match-expected-goal-totals/) on [2+2=11](https://2plus2equals11.com/);\n",
    "     +    [How StatsBomb Data Helps Measure Counter-Pressing](https://statsbomb.com/2018/05/how-statsbomb-data-helps-measure-counter-pressing/) for StatsBomb\n",
    "*    [Martin Eastwoood](https://twitter.com/penaltyblog) (Pena.lt/y)'s Expected Goals pieces [[link](https://pena.lt/y/category/expected-goals.html)];\n",
    "     +    [Expected Goals For All.](https://pena.lt/y/2014/02/12/expected-goals-for-all)\n",
    "     +    [Actual Goals Versus Expected Goals](https://pena.lt/y/2014/02/15/actual-goals-versus-expected-goals);\n",
    "     +    [Expected Goals Updated](https://pena.lt/y/2014/03/01/expected-goals-updated);\n",
    "     +    [Expected Goals: The Y Axis](https://pena.lt/y/2014/04/16/expected-goals-the-y-xis);\n",
    "     +    [Expected Goals And Exponential Decay](https://pena.lt/y/2014/04/22/expected-goals-and-exponential-decay);\n",
    "     +    [Expected Goals: Foot Shots Versus Headers](https://pena.lt/y/2014/08/28/expected-goals-foot-shots-versus-headers);\n",
    "     +    [Expected Goals And Support Vector Machines](https://pena.lt/y/2015/07/13/expected-goals-svm);\n",
    "     +    [Expected Goals and Uncertainty](https://pena.lt/y/2016/04/29/expected-goals-and-uncertainty); and\n",
    "     +    [Sharing xG Using Multi-touch Attribution Modelling](https://pena.lt/y/2019/11/23/multitouch-attributed-xg).\n",
    "*    [Garry Gelade](https://twitter.com/GarryGelade)'s Expected Goals pieces:\n",
    "     +    [Expected Goals and Unexpected Goals](https://web.archive.org/web/20200724125157/http://business-analytic.co.uk/blog/expected-goals-and-unexpected-goals/) (using WayBackMachine);\n",
    "     +    [Assessing Expected Goals Models. Part 1: Shots](https://web.archive.org/web/20200724125157/http://business-analytic.co.uk/blog/evaluating-expected-goals-models/) (using WayBackMachine);\n",
    "     +    [Assessing Expected Goals Models. Part 2: Anatomy of a Big Chance](https://web.archive.org/web/20200724125157/http://business-analytic.co.uk/blog/assessing-expected-goals-models-part-2-anatomy-of-a-big-chance/) (using WayBackMachine);\n",
    "*    [Introducing xGChain and xGBuildup](https://statsbomb.com/2018/08/introducing-xgchain-and-xgbuildup/) by [Thom Lawrence](https://twitter.com/lemonwatcher);\n",
    "*    [Quantifying finishing skill](https://statsbomb.com/2017/07/quantifying-finishing-skill/) by [Marek Kwiatkowski](https://twitter.com/statlurker);\n",
    "*    [The Dual Life of Expected Goals (Part 1)](https://statsbomb.com/2018/05/the-dual-life-of-expected-goals-part-1/) by [Mike L. Goodman](https://twitter.com/TheM_L_G);\n",
    "*    [A close look at my new Expected Goals Model](https://web.archive.org/web/20200320193539/http://11tegen11.net/2015/08/14/a-close-look-at-my-new-expected-goals-model/) by by [11tegen](https://twitter.com/11tegen11) ([Sander IJtsma](https://twitter.com/IJtsma)] (using WayBackMachine);\n",
    "*    [An analysis of different expected goals models](https://www.pinnacle.com/en/betting-articles/Soccer/expected-goals-model-analysis/MEP2N9VMG5CTW99D) by [Benjamin Cronin](https://twitter.com/PinnacleBen);\n",
    "*    [Expected Goals 3.0 Methodology](https://www.americansocceranalysis.com/home/2015/4/14/expected-goals-methodology) by [Matthias Kullowatz](https://twitter.com/mattyanselmo);\n",
    "*    [Explaining and Training Shot Quality](https://statsbomb.com/2016/04/explaining-and-training-shot-quality/) by [Ted Knutson](https://twitter.com/mixedknuts);\n",
    "*    [A simple Expected Goals model](https://cricketsavant.wordpress.com/2017/01/21/a-simple-expected-goals-model/) by Cricket Savant;\n",
    "*    [How we calculate Expected Goals (xG)](https://www.fantasyfootballfix.com/blog-index/how-we-calculate-expected-goals-xg/) by Fantasy Football Fix; and\n",
    "*    [Una mirada al Soccer Analytics usando R — Parte III](https://medium.com/datos-y-ciencia/una-mirada-al-soccer-analytics-usando-r-parte-iii-3bdff9cd3752) by [Dato Fútbol](https://twitter.com/DatoFutbol_cl) (Ismael Gómez Schmidt).\n",
    "\n",
    "##### News Articles\n",
    "*    [Liverpool sign up for StatsBomb 360: Ted Knutson explains why this stats revolution will change the game](https://www.skysports.com/football/news/11669/12248621/liverpool-sign-up-for-statsbomb-360-ted-knutson-explains-why-this-stats-revolution-will-change-the-game) (18/03/2021) by Adam Bate for Sky Sports News;\n",
    "*    [Man City’s Big Winter Signing Is a Former Hedge Fund Brain](https://www.bloombergquint.com/markets/man-city-s-big-winter-signing-is-a-former-hedge-fund-brain) (31/01/2021) by David Dellier and Adam Blenford for Bloomberg;\n",
    "*    [Man City land big signing in quest to be the best in data science](https://trainingground.guru/articles/man-city-land-big-signing-in-quest-to-be-the-best-in-data-science) (17/01/2021) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/);\n",
    "*    [Man City launch AI football competition with Google](https://trainingground.guru/articles/man-city-launch-ai-football-competition-with-google) (13/10/2020) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/);\n",
    "*    [Manchester City hire Huddersfield Town recruitment co-ordinator](https://trainingground.guru/articles/manchester-city-hire-huddersfield-recruitment-co-ordinator) (05/08/2020) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/);\n",
    "*    [Manchester City appoint Sisman to new role of Performance Physicist](https://trainingground.guru/articles/manchester-city-appoint-sisman-to-new-role-of-performance-physicist) (20/07/2020) by [Training Ground Guru](https://trainingground.guru/);\n",
    "*    [Prestidge promoted to top data science job at Man City](https://trainingground.guru/articles/prestidge-promoted-to-top-data-science-job-at-man-city) (02/01/2020) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/);\n",
    "*    [Man City Head of Data Insights leaves after six years](https://trainingground.guru/articles/man-city-head-of-data-insights-leaves-after-six-years) (10/11/2019) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/); and\n",
    "*    [Manchester City create new first team data science role](https://trainingground.guru/articles/manchester-city-create-new-first-team-data-science-role) (20/06/2019) by [Simon Austin](https://twitter.com/sport_simon) for [Training Ground Guru](https://trainingground.guru/).\n",
    "\n",
    "##### Books\n",
    "*    [The Numbers Game](https://www.amazon.co.uk/Numbers-Game-Everything-About-Football/) by [Chris Anderson](https://twitter.com/soccerquant) and [David Sally](https://twitter.com/DavidSally6);\n",
    "*    [Football Hackers](https://www.amazon.co.uk/Football-Hackers-Science-Data-Revolution/) by [Christoph Biermann](https://twitter.com/chbiermann);\n",
    "*    [Soccermatics](https://www.amazon.co.uk/Soccermatics-Mathematical-Adventures-Pro-Bloomsbury/dp/1472924142/ref=tmm_pap_swatch_0?_encoding=UTF8&qid=&sr=) by [David Sumpter](https://twitter.com/Soccermatics);\n",
    "*    [Soccernomics](https://www.amazon.co.uk/Soccernomics-England-Germany-France-Finally/) by Simon Kuper and [Stefan Szymanski](https://twitter.com/sszy);\n",
    "*    [Money and Football: A Soccernomics Guide ](https://www.amazon.co.uk/dp/B06XCKCVQR/) by Simon Kuper and [Stefan Szymanski](https://twitter.com/sszy); and\n",
    "*    [Data Analytics in Football](https://www.amazon.co.uk/Data-Analytics-Football-Daniel-Memmert/) by [Daniel Memmert](https://twitter.com/DMemmert) and Dominik Raabe.\n",
    "\n",
    "#### Videos\n",
    "For a YouTube playlist of videos collated around the topics of Expected Goals, see [[link](https://www.youtube.com/playlist?list=PL38nJNjpNpH_VPRZJrkaPZOJfyuIaZHUY)]. For a Tracking data in Football specific playlist, see [[link](https://www.youtube.com/playlist?list=PL38nJNjpNpH-UX0YVNu7oN5gAWQc2hq8F)].\n",
    "\n",
    "##### Webinars and Lectures\n",
    "*    Laurie Shaw's Metrica Sports Tracking data series for [Friends of Tracking](https://www.youtube.com/channel/UCUBFJYcag8j2rm_9HkrrA7w) (see the following for code [[link](https://github.com/Friends-of-Tracking-Data-FoTD/LaurieOnTracking)]):\n",
    "     +    [Introduction](https://www.youtube.com/watch?v=8TrleFklEsE);\n",
    "     +    [Measuring Physical Performance](https://www.youtube.com/watch?v=VX3T-4lB2o0);\n",
    "     +    [Pitch Control modelling](https://www.youtube.com/watch?v=5X1cSehLg6s); and\n",
    "     +    [Valuing Actions](https://www.youtube.com/watch?v=KXSLKwADXKI).\n",
    "*    [Demystifying Tracking data Sportlogiq webinar](https://www.youtube.com/watch?v=miEWHSTYvX4) by Sam Gregory and Devin Pleuler;\n",
    "*    [Will Spearman's masterclass in Pitch Control](https://www.youtube.com/watch?v=X9PrwPyolyU&list=PL38nJNjpNpH-l59NupDBW7oG7CmWBgp7Y) for Friends of Tracking;\n",
    "*    [How Tracking Data is Used in Football and What are the Future Challenges](https://www.youtube.com/watch?v=kHTq9cwdkGA) with Javier Fernández, Sudarshan 'Suds' Gopaladesikan, Laurie Shaw, Will Spearman and David Sumpter for Friends of Tracking.\n",
    "*    [Introduction to tracking data in football](https://www.youtube.com/watch?v=fYqEnoOV9Po) by David Sumpter for Friends of Tracking;\n",
    "*    [Learning to Watch Football: Self-Supervised Representations](https://vimeo.com/398489039/80d8dcfb58) for Tracking Data by Karun Singh. See accompanying blog post [[link](https://karun.in/blog/ssr-tracking-data.html)];\n",
    "*    David Sumpter's Expected Goals webinars for [Friends of Tracking](https://www.youtube.com/channel/UCUBFJYcag8j2rm_9HkrrA7w) (see the following for code [3xGModel](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/3xGModel.py), [4LinearRegression](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/4LinearRegression.py), [5xGModelFit.py](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/5xGModelFit.py), and [6MeasuresOfFit](https://github.com/Friends-of-Tracking-Data-FoTD/SoccermaticsForPython/blob/master/6MeasuresOfFit.py)):\n",
    "     +    [How to Build An Expected Goals Model 1: Data and Model](https://www.youtube.com/watch?v=bpjLyFyLlXs);\n",
    "     +    [How to Build An Expected Goals Model 2: Statistical fitting](https://www.youtube.com/watch?v=wHOgINJ5g54); and\n",
    "     +    [The Ultimate Guide to Expected Goals](https://www.youtube.com/watch?v=310_eW0hUqQ).\n",
    "*    ['Good practice in data visualisation'](https://www.youtube.com/watch?v=md0pdsWtq_o) webinar by Peter McKeever for Friends Of Tracking. See the following for code [[link](https://github.com/petermckeeverPerform/friends-of-tracking-viz-lecture)];\n",
    "*    [The Ultimate Guide to Expected Goals](https://www.youtube.com/watch?v=310_eW0hUqQ) David Sumpter for Friends of Tracking;\n",
    "*    [How to explain Expected Goals to a football player](https://www.youtube.com/watch?v=Xc6IG9-Dt18) by David Sumpter;\n",
    "*    [What is xG?](https://www.youtube.com/watch?v=zSaeaFcm1SY) by [Tifo Football](https://www.youtube.com/channel/UCGYYNGmyhZ_kwBF_lqqXdAQ);\n",
    "*    [Opta Expected Goals](https://www.youtube.com/watch?v=w7zPZsLGK18) by [The Analyst](https://www.youtube.com/user/optasports) (formally Opta);\n",
    "*    [What are Expected Goals?](https://www.youtube.com/watch?v=Xc6IG9-Dt18) by [David Sumpter](https://twitter.com/Soccermatics) and Axel Pershagen;\n",
    "*    [Anatomy of a Goal](https://www.youtube.com/watch?v=YJuHC7xXsGA) by [Numberphile](https://twitter.com/numberphile) [Brady Haran](https://twitter.com/BradyHaran));\n",
    "*    [Sam Green OptaPro Interview](https://www.youtube.com/watch?v=gHIY-MgDh_o);\n",
    "*    [How Did These Goals Go In? - We Explain How Goal Probability Works](https://www.youtube.com/watch?v=_vGhocyvKhA) by the Bundesliga;\n",
    "*    [Soccer Analytics: Expected Goals](https://www.youtube.com/watch?v=3rsDCxszCD0) by [Dan Altman](https://twitter.com/NYAsports); and\n",
    "*    [Anatomy of an Expected Goal](https://www.youtube.com/watch?v=mgHIx0LSrqM) by [11tegen](https://twitter.com/11tegen11) ([Sander IJtsma](https://twitter.com/IJtsma));\n",
    "*    [Anatomy of a Goal (with Sam Green)](https://www.youtube.com/watch?v=YJuHC7xXsGA) by Numberphile: \n",
    "*    [\"Is Our Model Learning What We Think It Is?\" Estimating the xG Impact of Actions in Football](https://www.youtube.com/watch?v=i7Ra4Qv4_m4) by [Tom Decroos](https://twitter.com/TomDecroos) from the 2019 StatsBomb Innovation in Football Conference;\n",
    "*    [Statsbomb Data Launch - Beyond Naive xG](https://www.youtube.com/watch?v=_AYY9XlWEB0) by [Ted Knutson](https://twitter.com/mixedknuts);\n",
    "*    [Karol Kurach - Google Research Football](https://www.youtube.com/watch?v=Va5dIxejqx0);\n",
    "*    [Karol Kurach (Google Brain) \"Google Research Football: Learning to Play Football with Deep RL](https://www.youtube.com/watch?v=lsN5y2frNig);\n",
    "*    [Google Research Football](https://www.youtube.com/watch?v=esQvSg2qeS0) by Piotr Stanczyk;\n",
    "*    [Google's AI Plays Football…For Science!](https://www.youtube.com/watch?v=Uk9p4Kk98_g) by Two Minute Papers;\n",
    "*    [Changing the soccer transfer market with big data](https://www.youtube.com/watch?v=UMeDP-lIBD8) by [Giels Brouwer](https://twitter.com/gielsbrouwer);\n",
    "*    [Soccermatics: how maths explains football](https://www.youtube.com/watch?v=Nv7JYtVbzvI) by [David Sumpter](https://twitter.com/Soccermatics);\n",
    "*    [Data Robot Opening Remarks & Keynote: Making Better Decisions, Faster](https://www.datarobot.com/recordings/ai-experience-emea-on-demand/ai-experience-opening-remarks-keynote/watch/090a6990db580257e9e6046fc48ab035/) with [Brian Prestidge](https://twitter.com/brianprestidge);\n",
    "\n",
    "##### Miscellaneous\n",
    "*    [Jeff Stelling xG rant](https://facebook.com/SoccerAM/videos/1740454985978128/); and\n",
    "*    [Craig Burley xG rant](https://www.youtube.com/watch?v=JBWKGij9Y5A).\n",
    "\n",
    "##### YouTube Channels\n",
    "*    [Friends of Tracking](https://www.youtube.com/channel/UCUBFJYcag8j2rm_9HkrrA7w) with [David Sumpter](https://twitter.com/Soccermatics), [Javier Fernández](https://twitter.com/JaviOnData), [Laurie Shaw](https://twitter.com/EightyFivePoint), [Sudarshan 'Suds' Gopaladesikan](https://twitter.com/suds_g), [Pascal Bauer](https://twitter.com/pascal_bauer), and [Fran Peralta](https://twitter.com/PeraltaFran23);\n",
    "*    [McKay Johns](https://www.youtube.com/channel/UCmqincDKps3syxvD4hbODSg);\n",
    "*    [Mark Glickman](https://www.youtube.com/channel/UC-gtC2WYRAr_4eYRIUb4ovg) – for NESSIS talks, uploaded to his personal channel. Old talks are available on his [Metacafe channel](https://www.metacafe.com/channels/Mark%20Glickman/). See the official website [[link](http://www.nessis.org/)];\n",
    "*    [42 Analytics](https://www.youtube.com/user/42analytics) – for SSAC conferences;\n",
    "*    [StatsBomb](https://www.youtube.com/channel/UCmZ2ArreL9muPvH49Gaw0Bw);\n",
    "*    [Opta](https://www.youtube.com/user/optasports) - including Opta Pro Forum talks; and\n",
    "*    [Tifo Football](https://www.youtube.com/channel/UCGYYNGmyhZ_kwBF_lqqXdAQ).\n",
    "\n",
    "#### Podcasts \n",
    "List of notable episodes:\n",
    "*    [All Stats Aren't We](https://open.spotify.com/show/22eR0UCjDdVXY2JTtjD3OI?si=kt_lY1m2QKukOvKvmWpsPA):\n",
    "     +    [Bonus Episode: David Sumpter - The Ten Equations that Rule the World](https://open.spotify.com/episode/2aWNiGHVH29qnXdrw12Iet?si=gU5__QfvRsCCxjE7XjcRhQ)\n",
    "*    [Analytics FC Podcast](https://analyticsfc.co.uk/podcast/):\n",
    "     +    [Episode 27: David Sumpter](https://open.spotify.com/episode/6gG4VY5hRlIio0smhgTnWh?si=meS7GqPxR4WXf2PGMPATZw)\n",
    "*    [The Conor J Show](https://open.spotify.com/show/2VeRpUoHzC7KN9zxB5N2iz?si=oSMPSpwbR7-IgxSzqlk6Ig):\n",
    "     +    [The Role Mathematics plays in Sports and Politics (Part 1) | David Sumpter | TCJS #11 (1/2)](https://open.spotify.com/episode/7BBAToNN9Mt0Ol8c9bDtGS?si=5YNtRObXQMu7xYBEgLgmbQ)\n",
    "*    [Expected Value](https://open.spotify.com/show/5xFeWbaaLFepY5n73SfWwr?si=yn23mqUpQa-mvcL6CYWpgA)\n",
    "     +    [NESSIS, Part 2 - Laurie Shaw & Sam Gregory](https://open.spotify.com/episode/42z1UFcfgpx17acCCg5rip?si=Pyu8gFJxRiej9fE15Gs89A)\n",
    "*    [The Football Collective Podcast](https://open.spotify.com/show/3fqNuhWi6hkagJ1U0UDJfe?si=e10JT2ACS86A3JXyO1AzGQ):\n",
    "     +    [S3 E1 | Sarthak Mondal speaks to Laurie Shaw about the advent of Data Science in Football The Football Collective Podcast](https://open.spotify.com/episode/1gJXuovD1L6VMimN5BtukS?si=Y-Ot43T8TluU7UEiSvReyg)\n",
    "*    [The Football Fanalytics Podcast](https://open.spotify.com/show/6JwWRPMaHfGicFBtl7nI3V?si=IwQ00tyTRPaBcW-0XLwS4w&nd=1)\n",
    "     +   [#1: What Did You Expect?](https://open.spotify.com/episode/3CkvTYcsLmNmD5BCIZhpvi?si=NaeVt2zOStm9EJ56n4EozQ)\n",
    "*    [The Football Pod](https://open.spotify.com/show/3QhwCTOvJN3AZqNalgjtnO?si=173ZCWfsTs-jktoS7Bz9XQ):\n",
    "     +    [Episode 3 with David Sumpter](https://open.spotify.com/episode/4mnDHbUo097JuC2lQiFijo?si=7abgc4_vRM21jSKff04rWg)\n",
    "*    [Football Today](https://open.spotify.com/show/1WRaXZgVlksph0IjsTNBaG?si=0zyUX59sTKqCRnq92SEylQ&nd=1)\n",
    "     +    [Manchester City Enters Data Arms Race With Liverpool](https://open.spotify.com/episode/311rLza8goz2b2SBORBORn?si=aqZX6ooOSfGkY3312jJozA)\n",
    "*    [Pinnacle Podcast](https://open.spotify.com/show/091oYrS0glFhP81fq32bpE?si=TmR79XGnRAm5987Zj33ImA):\n",
    "     +    [Serious About Betting: David Sumpter](https://open.spotify.com/episode/6Bt5L7wXDGvrSezYL2FU2O?si=IgQB1TArR9CBTnecZxb6qA)\n",
    "*    [The Scouted Football Podcast](https://open.spotify.com/show/4qYVKC8RlHCJrwrRCx0w6H?si=M6xgCGtdTjiy0wEl1e2CJw)\n",
    "     +    #56: Dominic Calvert-Lewin & Explaining Expected Goals - [Spotify](https://open.spotify.com/episode/37SlOJmtoviAKgNanq7Fxq?si=AAnRaCUOTw6FaVkreD5Rzg) and [YouTube](https://www.youtube.com/watch?v=EE_m3VBcASU) by [The Scouted Football Podcast](https://open.spotify.com/show/4qYVKC8RlHCJrwrRCx0w6H?si=M6xgCGtdTjiy0wEl1e2CJw).\n",
    "*    [Squawka Talker Football Podcast](https://open.spotify.com/show/7xqylrPDX54uo01n4erZQZ?si=XpMNQ43aQxKUa6QuB0dp2w):\n",
    "     +    [BIG interview with David Sumpter: Putting GPS trackers on under 10s football teams](https://open.spotify.com/episode/6c6vOWNhvUah7Mz01oiCgt?si=t7Sc0W8WRUS0ZnVWTaGt0g)\n",
    "*    [Tifo Podcast](https://open.spotify.com/show/06QIGhqK31Qw1UvfHzRIDA?si=eJzpmtMeSPWUDP9fQ-5pqA):\n",
    "     +    The Future of Stats: xG, xA - [Spotify](https://open.spotify.com/episode/7fPpKZSt2o9SSNynayROwd?si=WxuV2PFCQ7yRdNSE-QOZ6g) and [YouTube](https://www.youtube.com/watch?v=sNCeA27sDvI)\n",
    "*    [Trademate Sports](https://open.spotify.com/show/2LPzUrtsWvz5iSayEGeEQK?si=prrlKqiwQ7-bKIUtbemkeQ):\n",
    "     +    [Ep 87: Mathematics Professor David Sumpter & Trademate CEO Marius Norheim - Using Mathematics in...](https://open.spotify.com/episode/6jYhTHxujga5D9j37uQbLt?si=3gwKy27dRcOgVij5bYpGzA)\n",
    "*    [UCN/USF Sport Management - Sports Business Podcast](https://soundcloud.com/user-736114890):\n",
    "     +    [Kenneth Cortsen talks to Laurie Shaw from Harvard University](https://soundcloud.com/user-736114890/sport-data-analytics-in-football-kenneth-cortsen-talks-to-laurie-shaw-harvard-university)\n",
    "\n",
    "#### Tweets\n",
    "*    The benefits of including fake data in an Expected Goals model by David Sumpter [[link](https://twitter.com/Soccermatics/status/1260598182624575490)].\n",
    "\n",
    "\n",
    "### Data Science\n",
    "\n",
    "#### Mathematics\n",
    "*    [Find if a point lies inside a Circle](https://www.geeksforgeeks.org/find-if-a-point-lies-inside-or-on-circle/) by Utkarsh Trivedi\n",
    "*    [How to know if a point is inside a circle?](https://math.stackexchange.com/questions/198764/how-to-know-if-a-point-is-inside-a-circle)\n",
    "*    [Check whether a given point lies inside a triangle or not](https://www.geeksforgeeks.org/check-whether-a-given-point-lies-inside-a-triangle-or-not/)\n",
    "\n",
    "#### Classification Metrics\n",
    "\n",
    "##### General\n",
    "*    [Confusion Matrix, Accuracy, Specificity, Precision, and Recall](https://www.coursera.org/lecture/supervised-learning-classification/confusion-matrix-accuracy-specificity-precision-and-recall-e9U0e)\n",
    "*    [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "##### Overview\n",
    "*    [scikit-learn documentation classification metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "##### ROC AUC\n",
    "*    [Receiver operating characteristic Wiki](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "*    [Understanding AUC-ROC](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)\n",
    "*    [How to Use ROC Curves and Precision-Recall Curves for Classification in Python](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/) by Jason Brownlee\n",
    "*    [scikit-learn ROC AUC score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n",
    "*    [Intuition behind ROC-AUC score](https://towardsdatascience.com/intuition-behind-roc-auc-score-1456439d1f30)\n",
    "\n",
    "##### Log Loss\n",
    "*    [Log Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n",
    "*    [Accuracy, Recall, Precision, F-Score & Specificity, which to optimise on?](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124)\n",
    "*    [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "*    [Intuition behind Log-loss score](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a)\n",
    "*    [Log Loss or Cross-Entropy Cost Function in Logistic Regression](https://www.youtube.com/watch?v=MztgenIfGgM)\n",
    "*    [Lecture 6.4 — Logistic Regression | Cost Function]](https://www.youtube.com/watch?v=HIQlmHxI6-0) by Andrew Ng\n",
    "*    [Binary Cross Entropy/Log Loss for Binary Classification](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/)\n",
    "*    [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) by Jason Brownlee\n",
    "*    [What is Log Loss?](https://www.kaggle.com/dansbecker/what-is-log-loss) by Dan Becker\n",
    "\n",
    "#### Modeling\n",
    "\n",
    "##### Logistic Regression\n",
    "*    [scikit-learn Logistic Regression official docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "*    [Logistic Regression wiki](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression)\n",
    "*    [Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8) by StatQuest (Josh Starmer)\n",
    "*    [Logistic Regression for Machine Learning](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) by Jason Brownlee\n",
    "*    [Logistic Regression Tutorial for Machine Learning](https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/) by Jason Brownlee\n",
    "\n",
    "##### XGBoost\n",
    "*    [Greedy Function Approximation: A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) by Jerome H. Friedman\n",
    "*    [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754) by Tianqi Chen and Carlos Guestrin (the authors of XGBoost);\n",
    "*    [XGBoost GitHub repo](https://github.com/dmlc/xgboost);\n",
    "*    [Awesome XGBoost repo](https://github.com/dmlc/xgboost/tree/master/demo);\n",
    "*    [XGBoost official documentation](https://xgboost.readthedocs.io/en/latest/index.html). See the tutorials [[link](https://xgboost.readthedocs.io/en/latest/tutorials/index.html)]:\n",
    "     +    [Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "     +    [General Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters)\n",
    "*    [Gradient Boosting wiki](https://en.wikipedia.org/wiki/Gradient_boosting);\n",
    "*    [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/) by Jason Brownlee\n",
    "*    [How to Develop Your First XGBoost Model in Python](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/) by Jason Brownlee\n",
    "*    [How to Configure XGBoost for Imbalanced Classification](https://machinelearningmastery.com/xgboost-for-imbalanced-classification/) by Jason Brownlee\n",
    "*    [How to Visualize Gradient Boosting Decision Trees With XGBoost in Python](https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/) by Jason Brownlee\n",
    "*    [Story and Lessons Behind the Evolution of XGBoost](https://sites.google.com/site/nttrungmtwiki/home/it/data-science---python/xgboost/story-and-lessons-behind-the-evolution-of-xgboost) - brief history and backstory to the creation of XGBoost by Tianqi Chen;\n",
    "*    [XGBoost A Scalable Tree Boosting System](https://www.youtube.com/watch?v=Vly8xGnNiWs) - talk by Tianqi Chen at the LA Machine Learning Meetup Group on 02/06/2016;\n",
    "*    [Kaggle Winning Solution Xgboost Algorithm](https://www.youtube.com/watch?v=ufHo8vbk6g4) - talk by Tong He (author of the R XGBoost package) at the NYC Data Science Academy. Mo\n",
    "*    [Gradient Boosting Machine Learning](https://www.youtube.com/watch?v=wPqtzj5VZus) - talk by Professor Trevor Hastie\n",
    "*    [XGBoost](https://www.kaggle.com/alexisbcook/xgboost) lessson, part of the [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) course by [Kaggle](https://www.kaggle.com/)\n",
    "*    [Hyperparameter Optimization for Xgboost](https://www.youtube.com/watch?v=9HomdnM12o4) by Krish Naik\n",
    "*    [XGBoost in Python from Start to Finish](https://www.youtube.com/watch?v=GrJP9FLV3FE) by StatQuest (Josh Starmer)\n",
    "*    [XGBoost + k-fold CV + Feature Importance](https://www.kaggle.com/prashant111/xgboost-k-fold-cv-feature-importance) by Prashant Banerjee\n",
    "*    [A Guide on XGBoost hyperparameters tuning](https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning) by Prashant Banerjee\n",
    "*    [Hyperparameter Grid Search with XGBoost](https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost)\n",
    "*    [Hyperopt the Xgboost model](https://www.kaggle.com/yassinealouini/hyperopt-the-xgboost-model) by Yassine Alouini\n",
    "*    [Using XGBoost in Python](https://www.datacamp.com/community/tutorials/xgboost-in-python) by Manish Pathak\n",
    "*    [Getting started with XGBoost](https://blog.cambridgespark.com/getting-started-with-xgboost) by Kevin Lemagnen\n",
    "*    [A Beginner’s guide to XGBoost](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7) by George Seif\n",
    "*    [Boosting your Machine Learning Models Using XGBoost](https://heartbeat.fritz.ai/boosting-your-machine-learning-models-using-xgboost-d2cabb3e948f) by Derrick Mwiti\n",
    "*    [XGBoost Algorithm: Long May She Reign!](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d) by Vishal Morde\n",
    "*    [Gradient Boosting and XGBoost](https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5) by Gabriel Tseng\n",
    "*    [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d) by Prince Grover\n",
    "*    [HyperParameter Tuning — Hyperopt Bayesian Optimization for (Xgboost and Neural network)](https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9) by Tinu Rohith D\n",
    "*    [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) by Aarshay Jain\n",
    "\n",
    "\n",
    "##### CatBoost\n",
    "*    [CatBoost.ai](https://catboost.ai/)\n",
    "*    [Official CatBoost GitHub repo](https://github.com/catboost)\n",
    "*    [Mastering gradient boosting with CatBoost](https://www.youtube.com/watch?v=usdEWSDisS0) by [Anna Veronika Dorogush](https://ru.linkedin.com/in/anna-veronika-dorogush-08739637) at PyData London 2019\n",
    "*    [Mastering Fast Gradient Boosting on Google Colaboratory with free GPU](https://towardsdatascience.com/mastering-fast-gradient-boosting-on-google-colaboratory-with-free-gpu-65c1dd47d1c5) by [Anna Veronika Dorogush](https://ru.linkedin.com/in/anna-veronika-dorogush-08739637)\n",
    "\n",
    "#### Feature Interpretation\n",
    "\n",
    "##### SHAP\n",
    "*    [Official SHAP GitHub repo](https://github.com/slundberg/shap)\n",
    "*    [Official documentation](https://shap.readthedocs.io/en/latest/tabular_examples.html)\n",
    "     +    [Linear models](https://shap.readthedocs.io/en/latest/tabular_examples.html#linear-models)\n",
    "     +    [Tree-based models](https://shap.readthedocs.io/en/latest/tabular_examples.html#tree-based-models)\n",
    "*    [A Unified Approach to Interpreting Model Predictions](https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) by Scott M. Lundberg and Su-In Lee\n",
    "*    [True to the Model or True to the Data?](https://arxiv.org/pdf/2006.16234.pdf) by Hugh Chen, Joseph D. Janizek, Scott Lundberg, and Su-In Lee\n",
    "*    [Interpretable Machine Learning with XGBoost](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) by Scott Lundberg (creator the of SHAP library)\n",
    "*    [Explain Your Model with the SHAP Values](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d) by Dr. Dataman\n",
    "*    [SHAP Values Explained Exactly How You Wished Someone Explained to You](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)\n",
    "\n",
    "#### Visualisation\n",
    "*    [How to Make a Plot with Two Different Y-axis in Python with Matplotlib](https://cmdlinetips.com/2019/10/how-to-make-a-plot-with-two-different-y-axis-in-python-with-matplotlib/)\n",
    "*    https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
    "*    [Control color in seaborn heatmaps](https://www.python-graph-gallery.com/92-control-color-in-seaborn-heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [eddwebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "664px",
    "left": "1059px",
    "right": "20px",
    "top": "-2px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
